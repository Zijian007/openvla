{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45d164c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "877cd6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 17:20:44.356520: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-21 17:20:44.356552: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-21 17:20:44.358404: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-21 17:20:44.367613: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-21 17:20:45.258119: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mNo private macro file found! (macros.py:53)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mIt is recommended to use a private macro file (macros.py:54)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mTo setup, run: python /home/zijianwang/miniconda3/envs/openvla/lib/python3.10/site-packages/robosuite/scripts/setup_macros.py (macros.py:55)\n",
      "/home/zijianwang/miniconda3/envs/openvla/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, torch\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union, List\n",
    "import types\n",
    "import draccus\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from libero.libero import benchmark\n",
    "import cv2\n",
    "import wandb\n",
    "from control_utils import entropy_units\n",
    "\n",
    "# Append current directory so that interpreter can find experiments.robot\n",
    "sys.path.append(\"../..\")\n",
    "from experiments.robot.libero.libero_utils import (\n",
    "    get_libero_dummy_action,\n",
    "    get_libero_env,\n",
    "    get_libero_image,\n",
    "    quat2axisangle,\n",
    "    save_rollout_video_CoA,\n",
    ")\n",
    "from experiments.robot.openvla_utils import get_processor, get_input\n",
    "from experiments.robot.robot_utils import (\n",
    "    DATE_TIME,\n",
    "    get_action,\n",
    "    get_CoA,\n",
    "    get_image_resize_size,\n",
    "    get_model,\n",
    "    invert_gripper_action,\n",
    "    normalize_gripper_action,\n",
    "    set_seed_everywhere,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1289cf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GenerateConfig:\n",
    "    \"\"\"Configuration for OpenVLA inference on LIBERO tasks.\"\"\"\n",
    "    # fmt: off\n",
    "\n",
    "    #################################################################################################################\n",
    "    # Model-specific parameters\n",
    "    #################################################################################################################\n",
    "    model_family: str = \"openvla\"                    # Model family\n",
    "    pretrained_checkpoint: Union[str, Path] = \"/mnt/sda/home/zijianwang/openvla/FT_res/openvla-7b-finetuned-libero-10+libero_10_no_noops+b4+lr-0.0005+lora-r48+dropout-0.0--image_aug--2025-07-18_19-26-25\"     # Pretrained checkpoint path\n",
    "    load_in_8bit: bool = False                       # (For OpenVLA only) Load with 8-bit quantization\n",
    "    load_in_4bit: bool = False                       # (For OpenVLA only) Load with 4-bit quantization\n",
    "\n",
    "    center_crop: bool = True                         # Center crop? (if trained w/ random crop image aug)\n",
    "\n",
    "    #################################################################################################################\n",
    "    # LIBERO environment-specific parameters\n",
    "    #################################################################################################################\n",
    "    task_suite_name: str = \"libero_10\"             # Task suite. Options: libero_spatial, libero_object, libero_goal, libero_10, libero_90\n",
    "    num_steps_wait: int = 10                         # Number of steps to wait for objects to stabilize in sim\n",
    "    num_trials_per_task: int = 50                    # Number of rollouts per task\n",
    "\n",
    "    #################################################################################################################\n",
    "    # Utils\n",
    "    #################################################################################################################\n",
    "    run_id_note: Optional[str] = None                # Extra note to add in run ID for logging\n",
    "    local_log_dir: str = \"./experiments/logs\"        # Local directory for eval logs\n",
    "\n",
    "    use_wandb: bool = False                          # Whether to also log results in Weights & Biases\n",
    "    wandb_project: str = \"YOUR_WANDB_PROJECT\"        # Name of W&B project to log to (use default!)\n",
    "    wandb_entity: str = \"YOUR_WANDB_ENTITY\"          # Name of entity to log under\n",
    "\n",
    "    seed: int = 7                                    # Random Seed (for reproducibility)\n",
    "    device: str = \"cuda:1\"                           # Device to use for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f036e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "ACTION_DIM = 7\n",
    "\n",
    "def add_text_to_image(temp_img, num_act_units, CoA_step):\n",
    "    \"\"\"Add text overlay to image showing length and step number.\n",
    "    \n",
    "    Args:\n",
    "        temp_img (np.ndarray): Input image of shape (224, 224, 3)\n",
    "        num_act_units (int): Number of action units\n",
    "        CoA_step (int): Current step number\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Image with text overlay\n",
    "    \"\"\"\n",
    "    img = temp_img.copy()\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    text = f\"length: {num_act_units}, step: {CoA_step}\"\n",
    "    \n",
    "    # Get text size to position it in upper right\n",
    "    (text_width, text_height), _ = cv2.getTextSize(text, font, 0.5, 1)\n",
    "    \n",
    "    # Position text 10 pixels from right and top edges\n",
    "    text_x = img.shape[1] - text_width - 10\n",
    "    text_y = text_height + 10\n",
    "    \n",
    "    # Add white text with black outline for visibility\n",
    "    cv2.putText(img, text, (text_x, text_y), font, 0.5, (0,0,0), 2)\n",
    "    cv2.putText(img, text, (text_x, text_y), font, 0.5, (255,255,255), 1)\n",
    "    \n",
    "    return img\n",
    "\n",
    "def spilt_chain_to_units(chain: torch.Tensor, unnorm_key):\n",
    "    \"\"\"Split action chain into individual action units.\"\"\"\n",
    "    # Assert chain dimensions\n",
    "    assert chain.shape[0] == 1, f\"Expected batch size 1, got {chain.shape[0]}\"\n",
    "    unit_length = ACTION_DIM + 1  # Each unit has 7 action dims + 1 separator token\n",
    "    assert chain.shape[1] % unit_length == 0, f\"Chain length {chain.shape[1]} is not divisible by unit length {unit_length}\"\n",
    "    \n",
    "    # Split chain into units\n",
    "    num_units = chain.shape[1] // unit_length\n",
    "    units = []\n",
    "    for i in range(num_units):\n",
    "        start_idx = i * unit_length\n",
    "        end_idx = start_idx + unit_length\n",
    "        unit = chain[:, start_idx:end_idx]\n",
    "        units.append(unit)\n",
    "        assert unit[:,-1] == 32001, f\"Unit {i} does not end with separator token 32001\"\n",
    "    return units\n",
    "\n",
    "def process_action_unit(self, units: List[torch.Tensor], unnorm_key) -> List[np.ndarray]:\n",
    "    \"\"\"Process action units and convert to continuous actions.\"\"\"\n",
    "    processed_units = []\n",
    "    for unit in units:\n",
    "        # Extract predicted action tokens and translate into (normalized) continuous actions\n",
    "        predicted_action_token_ids = unit[0, :self.get_action_dim(unnorm_key)].cpu().numpy()\n",
    "        discretized_actions = self.vocab_size - predicted_action_token_ids\n",
    "        discretized_actions = np.clip(discretized_actions - 1, a_min=0, a_max=self.bin_centers.shape[0] - 1)\n",
    "        normalized_actions = self.bin_centers[discretized_actions]\n",
    "\n",
    "        # Unnormalize actions\n",
    "        action_norm_stats = self.get_action_stats(unnorm_key)\n",
    "        mask = action_norm_stats.get(\"mask\", np.ones_like(action_norm_stats[\"q01\"], dtype=bool))\n",
    "        action_high, action_low = np.array(action_norm_stats[\"q99\"]), np.array(action_norm_stats[\"q01\"])\n",
    "        action = np.where(\n",
    "            mask,\n",
    "            0.5 * (normalized_actions + 1) * (action_high - action_low) + action_low,\n",
    "            normalized_actions,\n",
    "        )\n",
    "        action = normalize_gripper_action(action, binarize=True)\n",
    "        action = invert_gripper_action(action)\n",
    "        processed_units.append(action)\n",
    "    return processed_units\n",
    "\n",
    "def predict_CoA(\n",
    "    self, input_ids: Optional[torch.LongTensor], unnorm_key: Optional[str], num_act_units: int = 100, top_k: int = 2, **kwargs: str\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"Predict Chain of Actions (CoA) using the VLA model.\"\"\"\n",
    "    # If the special empty token ('') does not already appear after the colon (':') token in the prompt\n",
    "    # (after \"OUT:\" or \"ASSISTANT:\"), insert it to match the inputs seen at training time\n",
    "    if not torch.all(input_ids[:, -1] == 29871):\n",
    "        input_ids = torch.cat(\n",
    "            (input_ids, torch.unsqueeze(torch.Tensor([29871]).long(), dim=0).to(input_ids.device)), dim=1\n",
    "        )\n",
    "    \n",
    "    # Run VLA inference\n",
    "    # print(f\"input_ids: {input_ids}\")\n",
    "\n",
    "    generated_dict = self.generate(input_ids, max_new_tokens=(self.get_action_dim(unnorm_key)+1)*num_act_units, **kwargs, do_sample=True, top_k = top_k, return_dict_in_generate=True, output_scores=True, output_attentions=True, output_hidden_states=True, output_logits = True)\n",
    "    logits = generated_dict[\"logits\"]\n",
    "    generated_ids = generated_dict[\"sequences\"]\n",
    "    # print(f\"generated_ids: {generated_ids}\")\n",
    "    assert (generated_ids.shape[1] - input_ids.shape[1]) % (ACTION_DIM + 1) == 0, f\"Action shape {generated_ids.shape} is not divisible by {ACTION_DIM + 1}\"\n",
    "    chain = generated_ids[:,input_ids.shape[1]:]\n",
    "    assert chain.shape[1] % (ACTION_DIM + 1) == 0, f\"Chain length {chain.shape} is not divisible by unit length {ACTION_DIM + 1}\"\n",
    "    \n",
    "    units: List[torch.Tensor] = spilt_chain_to_units(chain, unnorm_key)\n",
    "    processed_units: List[np.ndarray] = process_action_unit(self, units, unnorm_key)\n",
    "    return processed_units\n",
    "\n",
    "def predict_CoA_adaptively(\n",
    "    self, input_ids: Optional[torch.LongTensor], unnorm_key: Optional[str], num_act_units: int = 100, top_k: int = 2, **kwargs: str\n",
    ") -> dict[str, List[np.ndarray]]:\n",
    "    \"\"\"Predict Chain of Actions (CoA) using the VLA model.\"\"\"\n",
    "    # If the special empty token ('') does not already appear after the colon (':') token in the prompt\n",
    "    # (after \"OUT:\" or \"ASSISTANT:\"), insert it to match the inputs seen at training time\n",
    "    if not torch.all(input_ids[:, -1] == 29871):\n",
    "        input_ids = torch.cat(\n",
    "            (input_ids, torch.unsqueeze(torch.Tensor([29871]).long(), dim=0).to(input_ids.device)), dim=1\n",
    "        )\n",
    "    \n",
    "    # Run VLA inference\n",
    "    # print(f\"input_ids: {input_ids}\")\n",
    "\n",
    "    generated_dict = self.generate(input_ids, max_new_tokens=(self.get_action_dim(unnorm_key)+1)*num_act_units, **kwargs, do_sample=True, top_k = top_k, return_dict_in_generate=True, output_scores=True, output_attentions=True, output_hidden_states=True, output_logits = True)\n",
    "    logits = generated_dict[\"logits\"]\n",
    "    entropies: List[float] = entropy_units(logits)\n",
    "    # print(f\"entropies: {entropies}\")\n",
    "    generated_ids = generated_dict[\"sequences\"]\n",
    "    # print(f\"generated_ids: {generated_ids}\")\n",
    "    assert (generated_ids.shape[1] - input_ids.shape[1]) % (ACTION_DIM + 1) == 0, f\"Action shape {generated_ids.shape} is not divisible by {ACTION_DIM + 1}\"\n",
    "    chain = generated_ids[:,input_ids.shape[1]:]\n",
    "    assert chain.shape[1] % (ACTION_DIM + 1) == 0, f\"Chain length {chain.shape} is not divisible by unit length {ACTION_DIM + 1}\"\n",
    "    \n",
    "    units: List[torch.Tensor] = spilt_chain_to_units(chain, unnorm_key)\n",
    "    processed_units: List[np.ndarray] = process_action_unit(self, units, unnorm_key)\n",
    "    res = {\n",
    "        \"processed_units\": processed_units,\n",
    "        \"entropies\": entropies\n",
    "    }\n",
    "    return res\n",
    "\n",
    "def setup_model_and_config(cfg: GenerateConfig):\n",
    "    \"\"\"Setup and validate configuration, then load the model.\"\"\"\n",
    "    assert cfg.pretrained_checkpoint is not None, \"cfg.pretrained_checkpoint must not be None!\"\n",
    "    if \"image_aug\" in cfg.pretrained_checkpoint:\n",
    "        assert cfg.center_crop, \"Expecting `center_crop==True` because model was trained with image augmentations!\"\n",
    "    assert not (cfg.load_in_8bit and cfg.load_in_4bit), \"Cannot use both 8-bit and 4-bit quantization!\"\n",
    "\n",
    "    # Set random seed\n",
    "    set_seed_everywhere(cfg.seed)\n",
    "\n",
    "    cfg.unnorm_key = cfg.task_suite_name\n",
    "\n",
    "    # Load model\n",
    "    model = get_model(cfg)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def setup_logging_and_environment(cfg: GenerateConfig, model):\n",
    "    \"\"\"Setup logging and LIBERO environment.\"\"\"\n",
    "    # [OpenVLA] Check that the model contains the action un-normalization key\n",
    "    if cfg.model_family == \"openvla\":\n",
    "        # In some cases, the key must be manually modified (e.g. after training on a modified version of the dataset\n",
    "        # with the suffix \"_no_noops\" in the dataset name)\n",
    "        if cfg.unnorm_key not in model.norm_stats and f\"{cfg.unnorm_key}_no_noops\" in model.norm_stats:\n",
    "            cfg.unnorm_key = f\"{cfg.unnorm_key}_no_noops\"\n",
    "        assert cfg.unnorm_key in model.norm_stats, f\"Action un-norm key {cfg.unnorm_key} not found in VLA `norm_stats`!\"\n",
    "\n",
    "    # [OpenVLA] Get Hugging Face processor\n",
    "    processor = None\n",
    "    if cfg.model_family == \"openvla\":\n",
    "        processor = get_processor(cfg)\n",
    "\n",
    "    # Initialize local logging\n",
    "    run_id = f\"EVAL-{cfg.task_suite_name}-{cfg.model_family}-{DATE_TIME}\"\n",
    "    if cfg.run_id_note is not None:\n",
    "        run_id += f\"--{cfg.run_id_note}\"\n",
    "    os.makedirs(cfg.local_log_dir, exist_ok=True)\n",
    "    local_log_filepath = os.path.join(cfg.local_log_dir, run_id + \".txt\")\n",
    "    log_file = open(local_log_filepath, \"w\")\n",
    "    print(f\"Logging to local log file: {local_log_filepath}\")\n",
    "\n",
    "    # Initialize Weights & Biases logging as well\n",
    "    if cfg.use_wandb:\n",
    "        wandb.init(\n",
    "            entity=cfg.wandb_entity,\n",
    "            project=cfg.wandb_project,\n",
    "            name=run_id,\n",
    "        )\n",
    "\n",
    "    # Initialize LIBERO task suite\n",
    "    benchmark_dict = benchmark.get_benchmark_dict()\n",
    "    task_suite = benchmark_dict[cfg.task_suite_name]()\n",
    "    num_tasks_in_suite = task_suite.n_tasks\n",
    "    print(f\"Task suite: {cfg.task_suite_name}\")\n",
    "    log_file.write(f\"Task suite: {cfg.task_suite_name}\\n\")\n",
    "\n",
    "    # Get expected image dimensions\n",
    "    resize_size = get_image_resize_size(cfg)\n",
    "\n",
    "    return processor, log_file, task_suite, num_tasks_in_suite, resize_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db812be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Starting OpenVLA LIBERO Inference Demo\n",
      "[*] Loading model and setting up configuration...\n",
      "[*] Instantiating Pretrained VLA model\n",
      "[*] Loading in BF16 with Flash-Attention Enabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: <class 'transformers_modules.openvla-7b-finetuned-libero-10+libero_10_no_noops+b4+lr-0.0005+lora-r48+dropout-0.0--image_aug--2025-07-18_19-26-25.modeling_prismatic.OpenVLAForActionPrediction'>\n",
      "[*] Setting up logging and environment...\n",
      "Logging to local log file: ./experiments/logs/EVAL-libero_10-openvla-2025_07_21-17_21_05.txt\n",
      "[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Task suite: libero_10\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Main function to run the OpenVLA LIBERO inference demo.\"\"\"\n",
    "print(\"[*] Starting OpenVLA LIBERO Inference Demo\")\n",
    "\n",
    "# Initialize configuration\n",
    "cfg = GenerateConfig()\n",
    "\n",
    "# Setup model and configuration\n",
    "print(\"[*] Loading model and setting up configuration...\")\n",
    "model = setup_model_and_config(cfg)\n",
    "\n",
    "# Setup logging and environment\n",
    "print(\"[*] Setting up logging and environment...\")\n",
    "processor, log_file, task_suite, num_tasks_in_suite, resize_size = setup_logging_and_environment(cfg, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c93e111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add CoA prediction method to model\n",
    "import functools\n",
    "import types\n",
    "top_k = 1\n",
    "# 创建一个带默认top_k=2的函数\n",
    "predict_CoA_with_defaults = functools.partial(predict_CoA_adaptively, top_k=top_k)\n",
    "\n",
    "# 添加到model\n",
    "model.get_CoA = types.MethodType(predict_CoA_with_defaults, model) \n",
    "\n",
    "# model.get_CoA = types.MethodType(predict_CoA(top_k=2), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2ff7529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_steps(task_suite_name: str) -> int:\n",
    "    \"\"\"Get maximum steps for different task suites.\"\"\"\n",
    "    max_steps_dict = {\n",
    "        \"libero_spatial\": 220,  # longest training demo has 193 steps\n",
    "        \"libero_object\": 280,   # longest training demo has 254 steps\n",
    "        \"libero_goal\": 300,     # longest training demo has 270 steps\n",
    "        \"libero_10\": 550,       # longest training demo has 505 steps\n",
    "        \"libero_90\": 400,       # longest training demo has 373 steps\n",
    "    }\n",
    "    return max_steps_dict.get(task_suite_name, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e65dd7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 10) 50\n"
     ]
    }
   ],
   "source": [
    "print(range(num_tasks_in_suite), cfg.num_trials_per_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6c8b804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Starting evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning]: datasets path /mnt/sda/home/zijianwang/LIBERO/libero/libero/../datasets does not exist!\n",
      "[Warning]: datasets path /mnt/sda/home/zijianwang/LIBERO/libero/libero/../datasets does not exist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Task: put both the alphabet soup and the tomato sauce in the basket\n",
      "Starting episode 1...\n",
      "num_act_units: 2\n",
      "gripper action: -1.0, entropy: 0.0\n",
      "gripper action: -1.0, entropy: 0.0\n",
      "gripper action: -1.0, entropy: 0.0\n",
      "gripper action: -1.0, entropy: 0.0105\n",
      "total_length:14/550\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [02:51<2:20:20, 171.84s/it]\n",
      "  0%|          | 0/10 [02:54<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 63\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Prepare observations dict\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Note: OpenVLA does not take proprio state as input\u001b[39;00m\n\u001b[1;32m     57\u001b[0m observation \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_image\u001b[39m\u001b[38;5;124m\"\u001b[39m: img,\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mconcatenate(\n\u001b[1;32m     60\u001b[0m         (obs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrobot0_eef_pos\u001b[39m\u001b[38;5;124m\"\u001b[39m], quat2axisangle(obs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrobot0_eef_quat\u001b[39m\u001b[38;5;124m\"\u001b[39m]), obs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrobot0_gripper_qpos\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     61\u001b[0m     ),\n\u001b[1;32m     62\u001b[0m     }\n\u001b[0;32m---> 63\u001b[0m CoA_dict \u001b[38;5;241m=\u001b[39m \u001b[43mget_CoA\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_act_units\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_act_units\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     71\u001b[0m CoA \u001b[38;5;241m=\u001b[39m CoA_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessed_units\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     72\u001b[0m entropies \u001b[38;5;241m=\u001b[39m CoA_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentropies\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/mnt/sda/home/zijianwang/openvla/experiments/robot/robot_utils.py:80\u001b[0m, in \u001b[0;36mget_CoA\u001b[0;34m(cfg, model, obs, task_label, processor, num_act_units)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Queries the model to get an action.\"\"\"\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mmodel_family \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenvla\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 80\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mget_vla_CoA\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrained_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munnorm_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenter_crop\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcenter_crop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_act_units\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_act_units\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# assert action.shape == (ACTION_DIM,), f\"Action shape: {action.shape}\"\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# assert action.shape[1] % (ACTION_DIM + 1) == 0, f\"Action shape {action.shape} is not divisible by {ACTION_DIM + 1}\"\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected `model_family` found in config.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/sda/home/zijianwang/openvla/experiments/robot/openvla_utils.py:216\u001b[0m, in \u001b[0;36mget_vla_CoA\u001b[0;34m(vla, processor, base_vla_name, obs, task_label, unnorm_key, center_crop, num_act_units)\u001b[0m\n\u001b[1;32m    213\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(prompt, image)\u001b[38;5;241m.\u001b[39mto(vla\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# Get action.\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mvla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_CoA\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munnorm_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munnorm_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_act_units\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_act_units\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
      "Cell \u001b[0;32mIn[4], line 114\u001b[0m, in \u001b[0;36mpredict_CoA_adaptively\u001b[0;34m(self, input_ids, unnorm_key, num_act_units, top_k, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[1;32m    108\u001b[0m         (input_ids, torch\u001b[38;5;241m.\u001b[39munsqueeze(torch\u001b[38;5;241m.\u001b[39mTensor([\u001b[38;5;241m29871\u001b[39m])\u001b[38;5;241m.\u001b[39mlong(), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(input_ids\u001b[38;5;241m.\u001b[39mdevice)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    109\u001b[0m     )\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Run VLA inference\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# print(f\"input_ids: {input_ids}\")\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m generated_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[43munnorm_key\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnum_act_units\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m logits \u001b[38;5;241m=\u001b[39m generated_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    116\u001b[0m entropies: List[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m entropy_units(logits)\n",
      "File \u001b[0;32m~/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/openvla/lib/python3.10/site-packages/transformers/generation/utils.py:1622\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1614\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1615\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1616\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1617\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1618\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1619\u001b[0m     )\n\u001b[1;32m   1621\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1622\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1623\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1636\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1637\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1638\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1639\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1640\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1645\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1646\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/openvla/lib/python3.10/site-packages/transformers/generation/utils.py:2791\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2788\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2790\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2791\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2792\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2794\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2795\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2796\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2798\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2799\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/openvla-7b-finetuned-libero-10+libero_10_no_noops+b4+lr-0.0005+lora-r48+dropout-0.0--image_aug--2025-07-18_19-26-25/modeling_prismatic.py:330\u001b[0m, in \u001b[0;36mPrismaticForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, pixel_values, labels, inputs_embeds, past_key_values, use_cache, output_attentions, output_hidden_states, output_projector_features, return_dict)\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must provide `past_key_values` during cached generation!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected key `labels` provided during cached generation!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 330\u001b[0m     language_model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlanguage_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;66;03m# === Handle Unimodal Forward ===\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/openvla/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1208\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1205\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1208\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1215\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1221\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/openvla/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1018\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1007\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1008\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1009\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1015\u001b[0m         cache_position,\n\u001b[1;32m   1016\u001b[0m     )\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1018\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1028\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/openvla/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:755\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[1;32m    754\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 755\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_attention_layernorm\u001b[49m(hidden_states)\n\u001b[1;32m    756\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[1;32m    757\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py:1675\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1666\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1668\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[1;32m   1669\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1670\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1674\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1677\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1697\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:2017\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.ThreadTracer.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/openvla/lib/python3.10/site-packages/debugpy/_vendored/pydevd/_pydev_bundle/pydev_is_thread_alive.py:16\u001b[0m, in \u001b[0;36mis_thread_alive\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39m_handle\u001b[38;5;241m.\u001b[39mis_done()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(_temp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_stopped\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# Python 3.12 and earlier has this\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mis_thread_alive\u001b[39m(t):\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39m_is_stopped\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(_temp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_Thread__stopped\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# Python 2.x has this\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "print(\"[*] Starting evaluation...\")\n",
    "\"\"\"Run the main evaluation loop.\"\"\"\n",
    "# Start evaluation\n",
    "total_episodes, total_successes = 0, 0\n",
    "total_num_act_units = 40\n",
    "max_steps = get_max_steps(cfg.task_suite_name)\n",
    "for task_id in tqdm.tqdm(range(num_tasks_in_suite)):\n",
    "    if task_id != 0: \n",
    "        continue\n",
    "    # Get task\n",
    "    task = task_suite.get_task(task_id)\n",
    "    \n",
    "    # Get default LIBERO initial states\n",
    "    initial_states = task_suite.get_task_init_states(task_id)\n",
    "\n",
    "    # Initialize LIBERO environment and task description\n",
    "    env, task_description = get_libero_env(task, cfg.model_family, resolution=256)\n",
    "\n",
    "    # Start episodes\n",
    "    task_episodes, task_successes = 0, 0\n",
    "    for episode_idx in tqdm.tqdm(range(cfg.num_trials_per_task)):\n",
    "        if episode_idx != 1:  # Currently only running first episode\n",
    "            continue\n",
    "        print(f\"\\nTask: {task_description}\")\n",
    "        log_file.write(f\"\\nTask: {task_description}\\n\")\n",
    "\n",
    "        # Set initial states\n",
    "        obs = env.set_init_state(initial_states[episode_idx])\n",
    "\n",
    "        print(f\"Starting episode {episode_idx}...\")\n",
    "        log_file.write(f\"Starting episode {episode_idx}...\\n\")\n",
    "        num_act_episode, num_act_successes = 0, 0\n",
    "        for num_act_units in range(2, total_num_act_units):  # Try from 1 to 100 units\n",
    "            # if num_act_units != 20:\n",
    "            #     continue\n",
    "            print(f\"num_act_units: {num_act_units}\")\n",
    "            log_file.write(f\"num_act_units: {num_act_units}\\n\")\n",
    "            total_length = 0\n",
    "            replay_images = []\n",
    "            env.reset()\n",
    "            obs = env.set_init_state(initial_states[episode_idx])\n",
    "            CoA_step = 0\n",
    "            while total_length < max_steps + cfg.num_steps_wait:\n",
    "                try:\n",
    "                    # IMPORTANT: Do nothing for the first few timesteps because the simulator drops objects\n",
    "                    # and we need to wait for them to fall\n",
    "                    if total_length < cfg.num_steps_wait:\n",
    "                        obs, reward, done, info = env.step(get_libero_dummy_action(cfg.model_family))\n",
    "                        total_length += 1\n",
    "                        continue\n",
    "\n",
    "                    # Get preprocessed image\n",
    "                    img = get_libero_image(obs, resize_size) #np.array [224, 224, 3]\n",
    "                    # Prepare observations dict\n",
    "                    # Note: OpenVLA does not take proprio state as input\n",
    "                    observation = {\n",
    "                        \"full_image\": img,\n",
    "                        \"state\": np.concatenate(\n",
    "                            (obs[\"robot0_eef_pos\"], quat2axisangle(obs[\"robot0_eef_quat\"]), obs[\"robot0_gripper_qpos\"])\n",
    "                        ),\n",
    "                        }\n",
    "                    CoA_dict = get_CoA(\n",
    "                        cfg,\n",
    "                        model,\n",
    "                        observation,\n",
    "                        task_description,\n",
    "                        processor=processor,\n",
    "                        num_act_units=num_act_units,\n",
    "                    ) \n",
    "                    CoA = CoA_dict[\"processed_units\"]\n",
    "                    entropies = CoA_dict[\"entropies\"]\n",
    "                    # print(f\"entropies: {entropies}\")\n",
    "                    CoA_step += 1   #[array([0.002, -0.002, 0.000, -0.000, -0.000, -0.001, -1.000])]\n",
    "                    current_CoA_length = len(CoA)\n",
    "                    total_length = total_length + current_CoA_length\n",
    "                    for unit_idx, action in enumerate(CoA):\n",
    "                        print(f\"gripper action: {action[-1]}, entropy: {entropies[unit_idx]}\")\n",
    "                        obs, reward, done, info = env.step(action.tolist())\n",
    "                        temp_img = get_libero_image(obs, resize_size) # temp_img: np.ndarray, shape: (224, 224, 3)\n",
    "                        temp_img = add_text_to_image(temp_img, num_act_units, CoA_step)\n",
    "                        replay_images.append(temp_img)\n",
    "                    print(f\"total_length:{total_length}/{max_steps}\", end=\"\\r\")\n",
    "                    if done:\n",
    "                        print(f\"Success at task {task_id} -- episode {episode_idx} -- num_act_units:{num_act_units}, using t = {total_length}\")\n",
    "                        log_file.write(f\"Success at task {task_id} -- episode {episode_idx} -- num_act_units:{num_act_units}, using t = {total_length}\\n\")\n",
    "                        task_successes += 1\n",
    "                        num_act_successes += 1\n",
    "                        total_successes += 1    \n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"Caught exception: {e}\")\n",
    "                    log_file.write(f\"Caught exception: {e}\\n\")\n",
    "                    break\n",
    "            task_episodes += 1\n",
    "            num_act_episode += 1\n",
    "            total_episodes += 1\n",
    "            save_rollout_video_CoA(\n",
    "                    replay_images, episode_idx, success = done, task_description = task_description, log_file = log_file, num_act_units = num_act_units, task_id = task_id, task_suite_name = cfg.task_suite_name, top_k = top_k\n",
    "                )\n",
    "            # Log current results\n",
    "            print(f\"Success: {done}\")\n",
    "            print(f\"# episodes completed so far: {total_episodes}\")\n",
    "            print(f\"# successes: {total_successes} ({total_successes / total_episodes * 100:.1f}%)\")\n",
    "            log_file.write(f\"Success: {done}\\n\")\n",
    "            log_file.write(f\"# episodes completed so far: {total_episodes}\\n\")\n",
    "            log_file.write(f\"# successes: {total_successes} ({total_successes / total_episodes * 100:.1f}%)\\n\")\n",
    "            log_file.flush()\n",
    "\n",
    "        # Log final results\n",
    "        print(f\"Current task success rate: {float(task_successes) / float(task_episodes)}\")\n",
    "        print(f\"Current total success rate: {float(total_successes) / float(total_episodes)}\")\n",
    "        log_file.write(f\"Current task success rate: {float(task_successes) / float(task_episodes)}\\n\")\n",
    "        log_file.write(f\"Current total success rate: {float(total_successes) / float(total_episodes)}\\n\")\n",
    "        log_file.flush()\n",
    "    # Save local log file\n",
    "    log_file.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
