{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 23:55:35.023600: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-16 23:55:35.023644: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-16 23:55:35.025333: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-16 23:55:35.035716: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-16 23:55:35.942247: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mNo private macro file found! (macros.py:53)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mIt is recommended to use a private macro file (macros.py:54)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mTo setup, run: python /home/zijianwang/miniconda3/envs/openvla/lib/python3.10/site-packages/robosuite/scripts/setup_macros.py (macros.py:55)\n",
      "/home/zijianwang/miniconda3/envs/openvla/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import types\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union, List\n",
    "import draccus\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from libero.libero import benchmark\n",
    "import wandb\n",
    "\n",
    "# Append current directory so that interpreter can find experiments.robot\n",
    "sys.path.append(\"../..\")\n",
    "from experiments.robot.libero.libero_utils import (\n",
    "    get_libero_dummy_action,\n",
    "    get_libero_env,\n",
    "    get_libero_image,\n",
    "    quat2axisangle,\n",
    "    save_rollout_video,\n",
    "    save_rollout_video_CoA\n",
    ")\n",
    "from experiments.robot.openvla_utils import get_processor, get_input\n",
    "from experiments.robot.robot_utils import (\n",
    "    DATE_TIME,\n",
    "    get_action,\n",
    "    get_CoA,\n",
    "    get_image_resize_size,\n",
    "    get_model,\n",
    "    invert_gripper_action,\n",
    "    normalize_gripper_action,\n",
    "    set_seed_everywhere,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GenerateConfig:\n",
    "    \"\"\"Configuration for OpenVLA inference on LIBERO tasks.\"\"\"\n",
    "    # fmt: off\n",
    "\n",
    "    #################################################################################################################\n",
    "    # Model-specific parameters\n",
    "    #################################################################################################################\n",
    "    model_family: str = \"openvla\"                    # Model family\n",
    "    pretrained_checkpoint: Union[str, Path] = \"/mnt/sda/home/zijianwang/openvla/FT_res/openvla-7b+libero_10_no_noops+b4+lr-0.0005+lora-r24+dropout-0.0--image_aug--2025-07-14_23-42-29\"     # Pretrained checkpoint path\n",
    "    load_in_8bit: bool = False                       # (For OpenVLA only) Load with 8-bit quantization\n",
    "    load_in_4bit: bool = False                       # (For OpenVLA only) Load with 4-bit quantization\n",
    "\n",
    "    center_crop: bool = True                         # Center crop? (if trained w/ random crop image aug)\n",
    "\n",
    "    #################################################################################################################\n",
    "    # LIBERO environment-specific parameters\n",
    "    #################################################################################################################\n",
    "    task_suite_name: str = \"libero_10\"             # Task suite. Options: libero_spatial, libero_object, libero_goal, libero_10, libero_90\n",
    "    num_steps_wait: int = 10                         # Number of steps to wait for objects to stabilize in sim\n",
    "    num_trials_per_task: int = 50                    # Number of rollouts per task\n",
    "\n",
    "    #################################################################################################################\n",
    "    # Utils\n",
    "    #################################################################################################################\n",
    "    run_id_note: Optional[str] = None                # Extra note to add in run ID for logging\n",
    "    local_log_dir: str = \"./experiments/logs\"        # Local directory for eval logs\n",
    "\n",
    "    use_wandb: bool = False                          # Whether to also log results in Weights & Biases\n",
    "    wandb_project: str = \"YOUR_WANDB_PROJECT\"        # Name of W&B project to log to (use default!)\n",
    "    wandb_entity: str = \"YOUR_WANDB_ENTITY\"          # Name of entity to log under\n",
    "\n",
    "    seed: int = 7                                    # Random Seed (for reproducibility)\n",
    "    device: str = \"cuda:1\"                           # Device to use for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "ACTION_DIM = 7\n",
    "\n",
    "def spilt_chain_to_units(chain: torch.Tensor, unnorm_key):\n",
    "    \"\"\"Split action chain into individual action units.\"\"\"\n",
    "    # Assert chain dimensions\n",
    "    assert chain.shape[0] == 1, f\"Expected batch size 1, got {chain.shape[0]}\"\n",
    "    unit_length = ACTION_DIM + 1  # Each unit has 7 action dims + 1 separator token\n",
    "    assert chain.shape[1] % unit_length == 0, f\"Chain length {chain.shape[1]} is not divisible by unit length {unit_length}\"\n",
    "    \n",
    "    # Split chain into units\n",
    "    num_units = chain.shape[1] // unit_length\n",
    "    units = []\n",
    "    for i in range(num_units):\n",
    "        start_idx = i * unit_length\n",
    "        end_idx = start_idx + unit_length\n",
    "        unit = chain[:, start_idx:end_idx]\n",
    "        units.append(unit)\n",
    "        assert unit[:,-1] == 32001, f\"Unit {i} does not end with separator token 32001\"\n",
    "    return units\n",
    "\n",
    "\n",
    "def process_action_unit(self, units: List[torch.Tensor], unnorm_key) -> List[np.ndarray]:\n",
    "    \"\"\"Process action units and convert to continuous actions.\"\"\"\n",
    "    processed_units = []\n",
    "    for unit in units:\n",
    "        # Extract predicted action tokens and translate into (normalized) continuous actions\n",
    "        predicted_action_token_ids = unit[0, :self.get_action_dim(unnorm_key)].cpu().numpy()\n",
    "        discretized_actions = self.vocab_size - predicted_action_token_ids\n",
    "        discretized_actions = np.clip(discretized_actions - 1, a_min=0, a_max=self.bin_centers.shape[0] - 1)\n",
    "        normalized_actions = self.bin_centers[discretized_actions]\n",
    "\n",
    "        # Unnormalize actions\n",
    "        action_norm_stats = self.get_action_stats(unnorm_key)\n",
    "        mask = action_norm_stats.get(\"mask\", np.ones_like(action_norm_stats[\"q01\"], dtype=bool))\n",
    "        action_high, action_low = np.array(action_norm_stats[\"q99\"]), np.array(action_norm_stats[\"q01\"])\n",
    "        action = np.where(\n",
    "            mask,\n",
    "            0.5 * (normalized_actions + 1) * (action_high - action_low) + action_low,\n",
    "            normalized_actions,\n",
    "        )\n",
    "        action = normalize_gripper_action(action, binarize=True)\n",
    "        action = invert_gripper_action(action)\n",
    "        processed_units.append(action)\n",
    "    return processed_units\n",
    "\n",
    "\n",
    "def predict_CoA(\n",
    "    self, input_ids: Optional[torch.LongTensor], unnorm_key: Optional[str], num_act_units: int = 100, **kwargs: str\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"Predict Chain of Actions (CoA) using the VLA model.\"\"\"\n",
    "    # If the special empty token ('') does not already appear after the colon (':') token in the prompt\n",
    "    # (after \"OUT:\" or \"ASSISTANT:\"), insert it to match the inputs seen at training time\n",
    "    if not torch.all(input_ids[:, -1] == 29871):\n",
    "        input_ids = torch.cat(\n",
    "            (input_ids, torch.unsqueeze(torch.Tensor([29871]).long(), dim=0).to(input_ids.device)), dim=1\n",
    "        )\n",
    "    \n",
    "    # Run VLA inference\n",
    "    # print(f\"input_ids: {input_ids}\")\n",
    "\n",
    "    generated_ids = self.generate(input_ids, max_new_tokens=(self.get_action_dim(unnorm_key)+1)*num_act_units, **kwargs, do_sample=False)\n",
    "    # print(f\"generated_ids: {generated_ids}\")\n",
    "    assert (generated_ids.shape[1] - input_ids.shape[1]) % (ACTION_DIM + 1) == 0, f\"Action shape {generated_ids.shape} is not divisible by {ACTION_DIM + 1}\"\n",
    "    chain = generated_ids[:,input_ids.shape[1]:]\n",
    "    assert chain.shape[1] % (ACTION_DIM + 1) == 0, f\"Chain length {chain.shape} is not divisible by unit length {ACTION_DIM + 1}\"\n",
    "    \n",
    "    units: List[torch.Tensor] = spilt_chain_to_units(chain, unnorm_key)\n",
    "    processed_units: List[np.ndarray] = process_action_unit(self, units, unnorm_key)\n",
    "    return processed_units\n",
    "\n",
    "model.get_CoA = types.MethodType(predict_CoA, model)\n",
    "\n",
    "def get_max_steps(task_suite_name: str) -> int:\n",
    "    \"\"\"Get maximum steps for different task suites.\"\"\"\n",
    "    max_steps_dict = {\n",
    "        \"libero_spatial\": 220,  # longest training demo has 193 steps\n",
    "        \"libero_object\": 280,   # longest training demo has 254 steps\n",
    "        \"libero_goal\": 300,     # longest training demo has 270 steps\n",
    "        \"libero_10\": 520,       # longest training demo has 505 steps\n",
    "        \"libero_90\": 400,       # longest training demo has 373 steps\n",
    "    }\n",
    "    return max_steps_dict.get(task_suite_name, 300)\n",
    "\n",
    "\n",
    "def setup_model_and_config(cfg: GenerateConfig):\n",
    "    \"\"\"Setup and validate configuration, then load the model.\"\"\"\n",
    "    assert cfg.pretrained_checkpoint is not None, \"cfg.pretrained_checkpoint must not be None!\"\n",
    "    if \"image_aug\" in cfg.pretrained_checkpoint:\n",
    "        assert cfg.center_crop, \"Expecting `center_crop==True` because model was trained with image augmentations!\"\n",
    "    assert not (cfg.load_in_8bit and cfg.load_in_4bit), \"Cannot use both 8-bit and 4-bit quantization!\"\n",
    "\n",
    "    # Set random seed\n",
    "    set_seed_everywhere(cfg.seed)\n",
    "\n",
    "    cfg.unnorm_key = cfg.task_suite_name\n",
    "\n",
    "    # Load model\n",
    "    model = get_model(cfg)\n",
    "    \n",
    "    # Add CoA prediction method to model\n",
    "    model.get_CoA = types.MethodType(predict_CoA, model)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def setup_logging_and_environment(cfg: GenerateConfig, model):\n",
    "    \"\"\"Setup logging and LIBERO environment.\"\"\"\n",
    "    # [OpenVLA] Check that the model contains the action un-normalization key\n",
    "    if cfg.model_family == \"openvla\":\n",
    "        # In some cases, the key must be manually modified (e.g. after training on a modified version of the dataset\n",
    "        # with the suffix \"_no_noops\" in the dataset name)\n",
    "        if cfg.unnorm_key not in model.norm_stats and f\"{cfg.unnorm_key}_no_noops\" in model.norm_stats:\n",
    "            cfg.unnorm_key = f\"{cfg.unnorm_key}_no_noops\"\n",
    "        assert cfg.unnorm_key in model.norm_stats, f\"Action un-norm key {cfg.unnorm_key} not found in VLA `norm_stats`!\"\n",
    "\n",
    "    # [OpenVLA] Get Hugging Face processor\n",
    "    processor = None\n",
    "    if cfg.model_family == \"openvla\":\n",
    "        processor = get_processor(cfg)\n",
    "\n",
    "    # Initialize local logging\n",
    "    run_id = f\"EVAL-{cfg.task_suite_name}-{cfg.model_family}-{DATE_TIME}\"\n",
    "    if cfg.run_id_note is not None:\n",
    "        run_id += f\"--{cfg.run_id_note}\"\n",
    "    os.makedirs(cfg.local_log_dir, exist_ok=True)\n",
    "    local_log_filepath = os.path.join(cfg.local_log_dir, run_id + \".txt\")\n",
    "    log_file = open(local_log_filepath, \"w\")\n",
    "    print(f\"Logging to local log file: {local_log_filepath}\")\n",
    "\n",
    "    # Initialize Weights & Biases logging as well\n",
    "    if cfg.use_wandb:\n",
    "        wandb.init(\n",
    "            entity=cfg.wandb_entity,\n",
    "            project=cfg.wandb_project,\n",
    "            name=run_id,\n",
    "        )\n",
    "\n",
    "    # Initialize LIBERO task suite\n",
    "    benchmark_dict = benchmark.get_benchmark_dict()\n",
    "    task_suite = benchmark_dict[cfg.task_suite_name]()\n",
    "    num_tasks_in_suite = task_suite.n_tasks\n",
    "    print(f\"Task suite: {cfg.task_suite_name}\")\n",
    "    log_file.write(f\"Task suite: {cfg.task_suite_name}\\n\")\n",
    "\n",
    "    # Get expected image dimensions\n",
    "    resize_size = get_image_resize_size(cfg)\n",
    "\n",
    "    return processor, log_file, task_suite, num_tasks_in_suite, resize_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Starting OpenVLA LIBERO Inference Demo\n",
      "[*] Loading model and setting up configuration...\n",
      "[*] Instantiating Pretrained VLA model\n",
      "[*] Loading in BF16 with Flash-Attention Enabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: <class 'transformers_modules.openvla.openvla-7b.31f090d05236101ebfc381b61c674dd4746d4ce0.modeling_prismatic.OpenVLAForActionPrediction'>\n",
      "[*] Setting up logging and environment...\n",
      "Logging to local log file: ./experiments/logs/EVAL-libero_10-openvla-2025_07_16-23_55_58.txt\n",
      "[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Task suite: libero_10\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Main function to run the OpenVLA LIBERO inference demo.\"\"\"\n",
    "print(\"[*] Starting OpenVLA LIBERO Inference Demo\")\n",
    "\n",
    "# Initialize configuration\n",
    "cfg = GenerateConfig()\n",
    "\n",
    "# Setup model and configuration\n",
    "print(\"[*] Loading model and setting up configuration...\")\n",
    "model = setup_model_and_config(cfg)\n",
    "\n",
    "# Setup logging and environment\n",
    "print(\"[*] Setting up logging and environment...\")\n",
    "processor, log_file, task_suite, num_tasks_in_suite, resize_size = setup_logging_and_environment(cfg, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Starting evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning]: datasets path /mnt/sda/home/zijianwang/LIBERO/libero/libero/../datasets does not exist!\n",
      "[Warning]: datasets path /mnt/sda/home/zijianwang/LIBERO/libero/libero/../datasets does not exist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Task: put both the cream cheese box and the butter in the basket\n",
      "Starting episode 1...\n",
      "num_act_units:10\n",
      "total_length:0/10\n",
      "total_length:1/10\n",
      "total_length:2/10\n",
      "total_length:3/10\n",
      "total_length:4/10\n",
      "total_length:5/10\n",
      "total_length:6/10\n",
      "total_length:7/10\n",
      "total_length:8/10\n",
      "total_length:9/10\n",
      "total_length:530/520\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "  2%|▏         | 1/50 [03:11<2:36:24, 191.51s/it]\n",
      "100%|██████████| 10/10 [03:14<00:00, 19.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved rollout MP4 at path ./rollouts/libero_10/2025_07_16-23_55_58/task_id=1_put_both_the_cream_cheese_box_and_the_butter_in_th/epi=0--num_act=10--success=False.mp4\n",
      "[*] Evaluation completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "print(\"[*] Starting evaluation...\")\n",
    "\"\"\"Run the main evaluation loop.\"\"\"\n",
    "# Start evaluation\n",
    "total_episodes, total_successes = 0, 0\n",
    "max_steps = get_max_steps(cfg.task_suite_name)\n",
    "for task_id in tqdm.tqdm(range(num_tasks_in_suite)):\n",
    "    if task_id != 1: \n",
    "        continue\n",
    "        \n",
    "    # Get task\n",
    "    task = task_suite.get_task(task_id)\n",
    "\n",
    "    # Get default LIBERO initial states\n",
    "    initial_states = task_suite.get_task_init_states(task_id)\n",
    "\n",
    "    # Initialize LIBERO environment and task description\n",
    "    env, task_description = get_libero_env(task, cfg.model_family, resolution=256)\n",
    "\n",
    "    # Start episodes\n",
    "    task_episodes, task_successes = 0, 0\n",
    "    for episode_idx in tqdm.tqdm(range(cfg.num_trials_per_task)):\n",
    "        if episode_idx != 0:  # Currently only running first episode\n",
    "            break\n",
    "            \n",
    "        print(f\"\\nTask: {task_description}\")\n",
    "        log_file.write(f\"\\nTask: {task_description}\\n\")\n",
    "\n",
    "        print(f\"Starting episode {task_episodes+1}...\")\n",
    "        log_file.write(f\"Starting episode {task_episodes+1}...\\n\")\n",
    "\n",
    "        for num_act_units in range(10, 11):  # Try from 1 to 40 units\n",
    "            print(f\"num_act_units:{num_act_units}\")\n",
    "            total_length = 0\n",
    "            replay_images = []\n",
    "            \n",
    "            # Reset environment\n",
    "            env.reset()\n",
    "            obs = env.set_init_state(initial_states[episode_idx])\n",
    "\n",
    "            while total_length < max_steps + cfg.num_steps_wait:\n",
    "                try:\n",
    "                    # IMPORTANT: Do nothing for the first few timesteps because the simulator drops objects\n",
    "                    # and we need to wait for them to fall\n",
    "                    if total_length < cfg.num_steps_wait:\n",
    "                        obs, reward, done, info = env.step(get_libero_dummy_action(cfg.model_family))\n",
    "                        total_length += 1\n",
    "                        continue\n",
    "\n",
    "                    # Get preprocessed image\n",
    "                    img = get_libero_image(obs, resize_size)\n",
    "\n",
    "                    # Prepare observations dict\n",
    "                    # Note: OpenVLA does not take proprio state as input\n",
    "                    observation = {\n",
    "                        \"full_image\": img,\n",
    "                        \"state\": np.concatenate(\n",
    "                            (obs[\"robot0_eef_pos\"], quat2axisangle(obs[\"robot0_eef_quat\"]), obs[\"robot0_gripper_qpos\"])\n",
    "                        ),\n",
    "                    }\n",
    "                    next_id = model\n",
    "                    CoA: List[np.ndarray] = get_CoA(\n",
    "                        cfg,\n",
    "                        model,\n",
    "                        observation,\n",
    "                        task_description,\n",
    "                        processor=processor,\n",
    "                        num_act_units=num_act_units,\n",
    "                    )\n",
    "                    \n",
    "                    current_CoA_length = len(CoA)\n",
    "                    total_length = total_length + current_CoA_length\n",
    "                    print(f\"total_length:{total_length}/{max_steps}\", end=\"\\r\")\n",
    "                    \n",
    "                    for unit_idx, action in enumerate(CoA):\n",
    "                        obs, reward, done, info = env.step(action.tolist())\n",
    "                        temp_img = get_libero_image(obs, resize_size)\n",
    "                        replay_images.append(temp_img)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Caught exception: {e}\")\n",
    "                    log_file.write(f\"Caught exception: {e}\\n\")\n",
    "                    break\n",
    "\n",
    "            save_rollout_video_CoA(\n",
    "                replay_images, total_episodes, success=done, task_description=task_description, \n",
    "                log_file=log_file, num_act_units=num_act_units, task_id=task_id, task_suite_name=cfg.task_suite_name\n",
    "            )\n",
    "\n",
    "log_file.close()\n",
    "\n",
    "print(\"[*] Evaluation completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
