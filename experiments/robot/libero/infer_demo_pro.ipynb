{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 21:48:42.601543: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-08-14 21:48:42.601583: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-08-14 21:48:42.603138: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-08-14 21:48:42.611671: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-14 21:48:43.503795: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mNo private macro file found! (macros.py:53)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mIt is recommended to use a private macro file (macros.py:54)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mTo setup, run: python /home/zijianwang/miniconda3/envs/openvla/lib/python3.10/site-packages/robosuite/scripts/setup_macros.py (macros.py:55)\n",
      "/home/zijianwang/miniconda3/envs/openvla/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, torch\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union, List\n",
    "import types\n",
    "import draccus\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from libero.libero import benchmark\n",
    "import cv2\n",
    "import wandb\n",
    "\n",
    "# Append current directory so that interpreter can find experiments.robot\n",
    "sys.path.append(\"../..\")\n",
    "from experiments.robot.libero.libero_utils import (\n",
    "    get_libero_dummy_action,\n",
    "    get_libero_env,\n",
    "    get_libero_image,\n",
    "    quat2axisangle,\n",
    "    save_rollout_video_CoA,\n",
    ")\n",
    "from experiments.robot.openvla_utils import get_processor, get_input\n",
    "from experiments.robot.robot_utils import (\n",
    "    DATE_TIME,\n",
    "    get_action,\n",
    "    get_CoA,\n",
    "    get_image_resize_size,\n",
    "    get_model,\n",
    "    get_vla_via_lora,\n",
    "    invert_gripper_action,\n",
    "    normalize_gripper_action,\n",
    "    set_seed_everywhere,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GenerateConfig:\n",
    "    \"\"\"Configuration for OpenVLA inference on LIBERO tasks.\"\"\"\n",
    "    # fmt: off\n",
    "    # root_dir: str = \"/hdd/zijianwang\"\n",
    "    root_dir: str = \"/mnt/sda/home/zijianwang\"\n",
    "    #################################################################################################################\n",
    "    # Model-specific parameters\n",
    "    #################################################################################################################\n",
    "    model_family: str = \"openvla\"                    # Model family\n",
    "    pretrained_checkpoint: Union[str, Path] = os.path.join(root_dir, \"openvla/FT_res/openvla-7b-finetuned-libero-10+libero_10_no_noops+b4+lr-0.0005+lora-r48+dropout-0.0--image_aug--2025-07-18_19-26-25\")     # Pretrained checkpoint path\n",
    "    lora_path: str = os.path.join(root_dir, \"/mnt/sda/home/zijianwang/openvla/DPO_adapter_tmp_dir/openvla-7b+libero_10_no_noops+task1+b1+lr-0.0005+lora-r48+dropout-0.0--2025-08-14_21-29-54/ckpt-20\")\n",
    "    base_vla_path: str = os.path.join(root_dir, \"HF_CACHE/openvla-7b-finetuned-libero-10\")\n",
    "    load_in_8bit: bool = False                       # (For OpenVLA only) Load with 8-bit quantization\n",
    "    load_in_4bit: bool = False                       # (For OpenVLA only) Load with 4-bit quantization\n",
    "\n",
    "    center_crop: bool = True                         # Center crop? (if trained w/ random crop image aug)\n",
    "\n",
    "    #################################################################################################################\n",
    "    # LIBERO environment-specific parameters\n",
    "    #################################################################################################################\n",
    "    task_suite_name: str = \"libero_10\"             # Task suite. Options: libero_spatial, libero_object, libero_goal, libero_10, libero_90\n",
    "    num_steps_wait: int = 10                         # Number of steps to wait for objects to stabilize in sim\n",
    "    num_trials_per_task: int = 50                    # Number of rollouts per task\n",
    "    unnorm_key = task_suite_name\n",
    "\n",
    "    #################################################################################################################\n",
    "    # Utils\n",
    "    #################################################################################################################\n",
    "    run_id_note: Optional[str] = None                # Extra note to add in run ID for logging\n",
    "    local_log_dir: str = \"./experiments/logs\"        # Local directory for eval logs\n",
    "\n",
    "    use_wandb: bool = False                          # Whether to also log results in Weights & Biases\n",
    "    wandb_project: str = \"YOUR_WANDB_PROJECT\"        # Name of W&B project to log to (use default!)\n",
    "    wandb_entity: str = \"YOUR_WANDB_ENTITY\"          # Name of entity to log under\n",
    "\n",
    "    seed: int = 7                                    # Random Seed (for reproducibility)\n",
    "    device: str = \"cuda:2\"                           # Device to use for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "ACTION_DIM = 7\n",
    "\n",
    "def add_text_to_image(temp_img, num_act_units, CoA_step):\n",
    "    \"\"\"Add text overlay to image showing length and step number.\n",
    "    \n",
    "    Args:\n",
    "        temp_img (np.ndarray): Input image of shape (224, 224, 3)\n",
    "        num_act_units (int): Number of action units\n",
    "        CoA_step (int): Current step number\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Image with text overlay\n",
    "    \"\"\"\n",
    "    img = temp_img.copy()\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    text = f\"length: {num_act_units}, step: {CoA_step}\"\n",
    "    \n",
    "    # Get text size to position it in upper right\n",
    "    (text_width, text_height), _ = cv2.getTextSize(text, font, 0.5, 1)\n",
    "    \n",
    "    # Position text 10 pixels from right and top edges\n",
    "    text_x = img.shape[1] - text_width - 10\n",
    "    text_y = text_height + 10\n",
    "    \n",
    "    # Add white text with black outline for visibility\n",
    "    cv2.putText(img, text, (text_x, text_y), font, 0.5, (0,0,0), 2)\n",
    "    cv2.putText(img, text, (text_x, text_y), font, 0.5, (255,255,255), 1)\n",
    "    \n",
    "    return img\n",
    "\n",
    "def spilt_chain_to_units(chain: torch.Tensor, unnorm_key):\n",
    "    \"\"\"Split action chain into individual action units.\"\"\"\n",
    "    # Assert chain dimensions\n",
    "    assert chain.shape[0] == 1, f\"Expected batch size 1, got {chain.shape[0]}\"\n",
    "    unit_length = ACTION_DIM + 1  # Each unit has 7 action dims + 1 separator token\n",
    "    assert chain.shape[1] % unit_length == 0, f\"Chain length {chain.shape[1]} is not divisible by unit length {unit_length}\"\n",
    "    \n",
    "    # Split chain into units\n",
    "    num_units = chain.shape[1] // unit_length\n",
    "    units = []\n",
    "    for i in range(num_units):\n",
    "        start_idx = i * unit_length\n",
    "        end_idx = start_idx + unit_length\n",
    "        unit = chain[:, start_idx:end_idx]\n",
    "        units.append(unit)\n",
    "        # assert unit[:,-1] == 32001, f\"Unit {i} does not end with separator token 32001\"\n",
    "    return units\n",
    "\n",
    "\n",
    "def process_action_unit(self, units: List[torch.Tensor], unnorm_key) -> List[np.ndarray]:\n",
    "    \"\"\"Process action units and convert to continuous actions.\"\"\"\n",
    "    processed_units = []\n",
    "    for unit in units:\n",
    "        # Extract predicted action tokens and translate into (normalized) continuous actions\n",
    "        predicted_action_token_ids = unit[0, :self.get_action_dim(unnorm_key)].cpu().numpy()\n",
    "        discretized_actions = self.vocab_size - predicted_action_token_ids\n",
    "        discretized_actions = np.clip(discretized_actions - 1, a_min=0, a_max=self.bin_centers.shape[0] - 1)\n",
    "        normalized_actions = self.bin_centers[discretized_actions]\n",
    "\n",
    "        # Unnormalize actions\n",
    "        action_norm_stats = self.get_action_stats(unnorm_key)\n",
    "        mask = action_norm_stats.get(\"mask\", np.ones_like(action_norm_stats[\"q01\"], dtype=bool))\n",
    "        action_high, action_low = np.array(action_norm_stats[\"q99\"]), np.array(action_norm_stats[\"q01\"])\n",
    "        action = np.where(\n",
    "            mask,\n",
    "            0.5 * (normalized_actions + 1) * (action_high - action_low) + action_low,\n",
    "            normalized_actions,\n",
    "        )\n",
    "        action = normalize_gripper_action(action, binarize=True)\n",
    "        action = invert_gripper_action(action)\n",
    "        processed_units.append(action)\n",
    "    return processed_units\n",
    "\n",
    "\n",
    "def predict_CoA(\n",
    "    self, input_ids: Optional[torch.LongTensor], unnorm_key: Optional[str], num_act_units: int = 100, top_k: int = 2, **kwargs: str\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"Predict Chain of Actions (CoA) using the VLA model.\"\"\"\n",
    "    # If the special empty token ('') does not already appear after the colon (':') token in the prompt\n",
    "    # (after \"OUT:\" or \"ASSISTANT:\"), insert it to match the inputs seen at training time\n",
    "    if not torch.all(input_ids[:, -1] == 29871):\n",
    "        input_ids = torch.cat(\n",
    "            (input_ids, torch.unsqueeze(torch.Tensor([29871]).long(), dim=0).to(input_ids.device)), dim=1\n",
    "        )\n",
    "    \n",
    "    # Run VLA inference\n",
    "    # print(f\"input_ids: {input_ids}\")\n",
    "\n",
    "    generated_ids = self.generate(input_ids, max_new_tokens=(self.get_action_dim(unnorm_key)+1)*num_act_units, **kwargs, do_sample=True, top_k = top_k)\n",
    "    # print(f\"generated_ids: {generated_ids}\")\n",
    "    assert (generated_ids.shape[1] - input_ids.shape[1]) % (ACTION_DIM + 1) == 0, f\"Action shape {generated_ids.shape} is not divisible by {ACTION_DIM + 1}\"\n",
    "    chain = generated_ids[:,input_ids.shape[1]:]\n",
    "    assert chain.shape[1] % (ACTION_DIM + 1) == 0, f\"Chain length {chain.shape} is not divisible by unit length {ACTION_DIM + 1}\"\n",
    "    \n",
    "    units: List[torch.Tensor] = spilt_chain_to_units(chain, unnorm_key)\n",
    "    processed_units: List[np.ndarray] = process_action_unit(self, units, unnorm_key)\n",
    "    return processed_units\n",
    "\n",
    "def setup_model_and_config(cfg: GenerateConfig):\n",
    "    \"\"\"Setup and validate configuration, then load the model.\"\"\"\n",
    "    assert cfg.pretrained_checkpoint is not None, \"cfg.pretrained_checkpoint must not be None!\"\n",
    "    if \"image_aug\" in cfg.pretrained_checkpoint:\n",
    "        assert cfg.center_crop, \"Expecting `center_crop==True` because model was trained with image augmentations!\"\n",
    "    assert not (cfg.load_in_8bit and cfg.load_in_4bit), \"Cannot use both 8-bit and 4-bit quantization!\"\n",
    "\n",
    "    # Set random seed\n",
    "    set_seed_everywhere(cfg.seed)\n",
    "\n",
    "    cfg.unnorm_key = cfg.task_suite_name\n",
    "\n",
    "    # Load model\n",
    "    model = get_model(cfg)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def setup_logging_and_environment(cfg: GenerateConfig, model):\n",
    "    \"\"\"Setup logging and LIBERO environment.\"\"\"\n",
    "    # [OpenVLA] Check that the model contains the action un-normalization key\n",
    "    if cfg.model_family == \"openvla\":\n",
    "        # In some cases, the key must be manually modified (e.g. after training on a modified version of the dataset\n",
    "        # with the suffix \"_no_noops\" in the dataset name)\n",
    "        if cfg.unnorm_key not in model.norm_stats and f\"{cfg.unnorm_key}_no_noops\" in model.norm_stats:\n",
    "            cfg.unnorm_key = f\"{cfg.unnorm_key}_no_noops\"\n",
    "        assert cfg.unnorm_key in model.norm_stats, f\"Action un-norm key {cfg.unnorm_key} not found in VLA `norm_stats`!\"\n",
    "\n",
    "    # [OpenVLA] Get Hugging Face processor\n",
    "    processor = None\n",
    "    if cfg.model_family == \"openvla\":\n",
    "        processor = get_processor(cfg)\n",
    "\n",
    "    # Initialize local logging\n",
    "    run_id = f\"EVAL-{cfg.task_suite_name}-{cfg.model_family}-{DATE_TIME}\"\n",
    "    if cfg.run_id_note is not None:\n",
    "        run_id += f\"--{cfg.run_id_note}\"\n",
    "    os.makedirs(cfg.local_log_dir, exist_ok=True)\n",
    "    local_log_filepath = os.path.join(cfg.local_log_dir, run_id + \".txt\")\n",
    "    log_file = open(local_log_filepath, \"w\")\n",
    "    print(f\"Logging to local log file: {local_log_filepath}\")\n",
    "\n",
    "    # Initialize Weights & Biases logging as well\n",
    "    if cfg.use_wandb:\n",
    "        wandb.init(\n",
    "            entity=cfg.wandb_entity,\n",
    "            project=cfg.wandb_project,\n",
    "            name=run_id,\n",
    "        )\n",
    "\n",
    "    # Initialize LIBERO task suite\n",
    "    benchmark_dict = benchmark.get_benchmark_dict()\n",
    "    task_suite = benchmark_dict[cfg.task_suite_name]()\n",
    "    num_tasks_in_suite = task_suite.n_tasks\n",
    "    print(f\"Task suite: {cfg.task_suite_name}\")\n",
    "    log_file.write(f\"Task suite: {cfg.task_suite_name}\\n\")\n",
    "\n",
    "    # Get expected image dimensions\n",
    "    resize_size = get_image_resize_size(cfg)\n",
    "\n",
    "    return processor, log_file, task_suite, num_tasks_in_suite, resize_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Starting OpenVLA LIBERO Inference Demo\n",
      "[*] Loading model and setting up configuration...\n",
      "[*] Instantiating Pretrained VLA model\n",
      "[*] Loading in BF16 with Flash-Attention Enabled\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">08/14 [21:49:15] </span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> | &gt;&gt; Expected `<span style=\"color: #808000; text-decoration-color: #808000\">transformers</span>==<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.40</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>` and `<span style=\"color: #808000; text-decoration-color: #808000\">tokenizers</span>==<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.19</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>`   <a href=\"file:///home/zijianwang/.cache/huggingface/modules/transformers_modules/openvla/openvla-7b/31f090d05236101ebfc381b61c674dd4746d4ce0/modeling_prismatic.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">modeling_prismatic.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/zijianwang/.cache/huggingface/modules/transformers_modules/openvla/openvla-7b/31f090d05236101ebfc381b61c674dd4746d4ce0/modeling_prismatic.py#228\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">228</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         but got `<span style=\"color: #808000; text-decoration-color: #808000\">transformers</span>==<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.53</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>` and `<span style=\"color: #808000; text-decoration-color: #808000\">tokenizers</span>==<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.21</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>`; there  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                         </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         might be inference-time regressions due to dependency changes.  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                         </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         If in doubt, pleaseuse the above versions.                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                         </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m08/14 [21:49:15]\u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m | >> Expected `\u001b[33mtransformers\u001b[0m==\u001b[1;36m4.40\u001b[0m.\u001b[1;36m1\u001b[0m` and `\u001b[33mtokenizers\u001b[0m==\u001b[1;36m0.19\u001b[0m.\u001b[1;36m1\u001b[0m`   \u001b]8;id=125711;file:///home/zijianwang/.cache/huggingface/modules/transformers_modules/openvla/openvla-7b/31f090d05236101ebfc381b61c674dd4746d4ce0/modeling_prismatic.py\u001b\\\u001b[2mmodeling_prismatic.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=153422;file:///home/zijianwang/.cache/huggingface/modules/transformers_modules/openvla/openvla-7b/31f090d05236101ebfc381b61c674dd4746d4ce0/modeling_prismatic.py#228\u001b\\\u001b[2m228\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                 \u001b[0m         but got `\u001b[33mtransformers\u001b[0m==\u001b[1;36m4.53\u001b[0m.\u001b[1;36m3\u001b[0m` and `\u001b[33mtokenizers\u001b[0m==\u001b[1;36m0.21\u001b[0m.\u001b[1;36m2\u001b[0m`; there  \u001b[2m                         \u001b[0m\n",
       "\u001b[2;36m                 \u001b[0m         might be inference-time regressions due to dependency changes.  \u001b[2m                         \u001b[0m\n",
       "\u001b[2;36m                 \u001b[0m         If in doubt, pleaseuse the above versions.                      \u001b[2m                         \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 10.34it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Main function to run the OpenVLA LIBERO inference demo.\"\"\"\n",
    "print(\"[*] Starting OpenVLA LIBERO Inference Demo\")\n",
    "\n",
    "# Initialize configuration\n",
    "cfg = GenerateConfig()\n",
    "\n",
    "# Setup model and configuration\n",
    "print(\"[*] Loading model and setting up configuration...\")\n",
    "# model = setup_model_and_config(cfg)\n",
    "\n",
    "model = get_vla_via_lora(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Setting up logging and environment...\n",
      "Logging to local log file: ./experiments/logs/EVAL-libero_10-openvla-2025_08_14-21_49_05.txt\n",
      "[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Task suite: libero_10\n"
     ]
    }
   ],
   "source": [
    "# Setup logging and environment\n",
    "print(\"[*] Setting up logging and environment...\")\n",
    "processor, log_file, task_suite, num_tasks_in_suite, resize_size = setup_logging_and_environment(cfg, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add CoA prediction method to model\n",
    "import functools\n",
    "import types\n",
    "top_k = 1\n",
    "# 创建一个带默认top_k=2的函数\n",
    "predict_CoA_with_defaults = functools.partial(predict_CoA, top_k=top_k)\n",
    "\n",
    "# 添加到model\n",
    "model.get_CoA = types.MethodType(predict_CoA_with_defaults, model) \n",
    "\n",
    "# model.get_CoA = types.MethodType(predict_CoA(top_k=2), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_steps(task_suite_name: str) -> int:\n",
    "    \"\"\"Get maximum steps for different task suites.\"\"\"\n",
    "    max_steps_dict = {\n",
    "        \"libero_spatial\": 220,  # longest training demo has 193 steps\n",
    "        \"libero_object\": 280,   # longest training demo has 254 steps\n",
    "        \"libero_goal\": 300,     # longest training demo has 270 steps\n",
    "        \"libero_10\": 200,       # longest training demo has 505 steps\n",
    "        \"libero_90\": 400,       # longest training demo has 373 steps\n",
    "    }\n",
    "    return max_steps_dict.get(task_suite_name, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 10) 50\n"
     ]
    }
   ],
   "source": [
    "print(range(num_tasks_in_suite), cfg.num_trials_per_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Starting evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning]: datasets path /mnt/sda/home/zijianwang/LIBERO/libero/libero/../datasets does not exist!\n",
      "[Warning]: datasets path /mnt/sda/home/zijianwang/LIBERO/libero/libero/../datasets does not exist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Task: put both the cream cheese box and the butter in the basket\n",
      "Starting episode 1...\n",
      "num_act_units: 13\n",
      "total_length:218/200\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 50/50 [02:09<00:00,  2.59s/it]\n",
      "100%|██████████| 10/10 [02:15<00:00, 13.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved rollout MP4 at path ./rollouts/libero_10/2025_08_14-21_49_05/task_id=1_put_both_the_cream_cheese_box_and_the_butter_in_th/epi=1--num_act=13--top_k=1--success=False.mp4\n",
      "Success: False\n",
      "# episodes completed so far: 1\n",
      "# successes: 0 (0.0%)\n",
      "Current task success rate: 0.0\n",
      "Current total success rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "print(\"[*] Starting evaluation...\")\n",
    "\"\"\"Run the main evaluation loop.\"\"\"\n",
    "# Start evaluation\n",
    "total_episodes, total_successes = 0, 0\n",
    "total_num_act_units = 40\n",
    "max_steps = get_max_steps(cfg.task_suite_name)\n",
    "for task_id in tqdm.tqdm(range(num_tasks_in_suite)):\n",
    "    if task_id != 1: \n",
    "        continue\n",
    "    # Get task\n",
    "    task = task_suite.get_task(task_id)\n",
    "    \n",
    "    # Get default LIBERO initial states\n",
    "    initial_states = task_suite.get_task_init_states(task_id)\n",
    "\n",
    "    # Initialize LIBERO environment and task description\n",
    "    env, task_description = get_libero_env(task, cfg.model_family, resolution=256)\n",
    "\n",
    "    # Start episodes\n",
    "    task_episodes, task_successes = 0, 0\n",
    "    for episode_idx in tqdm.tqdm(range(cfg.num_trials_per_task)):\n",
    "        if episode_idx != 1:  # Currently only running first episode\n",
    "            continue\n",
    "        print(f\"\\nTask: {task_description}\")\n",
    "        log_file.write(f\"\\nTask: {task_description}\\n\")\n",
    "\n",
    "        # Set initial states\n",
    "        obs = env.set_init_state(initial_states[episode_idx])\n",
    "\n",
    "        print(f\"Starting episode {episode_idx}...\")\n",
    "        log_file.write(f\"Starting episode {episode_idx}...\\n\")\n",
    "        num_act_episode, num_act_successes = 0, 0\n",
    "        for num_act_units in range(1, total_num_act_units):  # Try from 1 to 100 units\n",
    "            if num_act_units != 13:\n",
    "                continue\n",
    "            print(f\"num_act_units: {num_act_units}\")\n",
    "            log_file.write(f\"num_act_units: {num_act_units}\\n\")\n",
    "            total_length = 0\n",
    "            replay_images = []\n",
    "            env.reset()\n",
    "            obs = env.set_init_state(initial_states[episode_idx])\n",
    "            CoA_step = 0\n",
    "            while total_length < max_steps + cfg.num_steps_wait:\n",
    "                try:\n",
    "                    # IMPORTANT: Do nothing for the first few timesteps because the simulator drops objects\n",
    "                    # and we need to wait for them to fall\n",
    "                    if total_length < cfg.num_steps_wait:\n",
    "                        obs, reward, done, info = env.step(get_libero_dummy_action(cfg.model_family))\n",
    "                        total_length += 1\n",
    "                        continue\n",
    "\n",
    "                    # Get preprocessed image\n",
    "                    img = get_libero_image(obs, resize_size) #np.array [224, 224, 3]\n",
    "                    # Prepare observations dict\n",
    "                    # Note: OpenVLA does not take proprio state as input\n",
    "                    observation = {\n",
    "                        \"full_image\": img,\n",
    "                        \"state\": np.concatenate(\n",
    "                            (obs[\"robot0_eef_pos\"], quat2axisangle(obs[\"robot0_eef_quat\"]), obs[\"robot0_gripper_qpos\"])\n",
    "                        ),\n",
    "                        }\n",
    "                    CoA:List[np.ndarray] = get_CoA(\n",
    "                        cfg,\n",
    "                        model,\n",
    "                        observation,\n",
    "                        task_description,\n",
    "                        processor=processor,\n",
    "                        num_act_units=num_act_units,\n",
    "                    ) \n",
    "                    CoA_step += 1   #[array([0.002, -0.002, 0.000, -0.000, -0.000, -0.001, -1.000])]\n",
    "                    current_CoA_length = len(CoA)\n",
    "                    total_length = total_length + current_CoA_length\n",
    "                    print(f\"total_length:{total_length}/{max_steps}\", end=\"\\r\")\n",
    "                    for unit_idx, action in enumerate(CoA):\n",
    "                        obs, reward, done, info = env.step(action.tolist())\n",
    "                        temp_img = get_libero_image(obs, resize_size) #temp_img: np.ndarray, shape: (224, 224, 3)\n",
    "                        temp_img = add_text_to_image(temp_img, num_act_units, CoA_step)\n",
    "                        replay_images.append(temp_img)\n",
    "                    if done:\n",
    "                        print(f\"Success at task {task_id} -- episode {episode_idx} -- num_act_units:{num_act_units}, using t = {total_length}\")\n",
    "                        log_file.write(f\"Success at task {task_id} -- episode {episode_idx} -- num_act_units:{num_act_units}, using t = {total_length}\\n\")\n",
    "                        task_successes += 1\n",
    "                        num_act_successes += 1\n",
    "                        total_successes += 1    \n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"Caught exception: {e}\")\n",
    "                    log_file.write(f\"Caught exception: {e}\\n\")\n",
    "                    break\n",
    "            task_episodes += 1\n",
    "            num_act_episode += 1\n",
    "            total_episodes += 1\n",
    "            save_rollout_video_CoA(\n",
    "                    replay_images, episode_idx, success=done, task_description=task_description, log_file=log_file, num_act_units=num_act_units, task_id = task_id, task_suite_name = cfg.task_suite_name, top_k=top_k\n",
    "                )\n",
    "            # Log current results\n",
    "            print(f\"Success: {done}\")\n",
    "            print(f\"# episodes completed so far: {total_episodes}\")\n",
    "            print(f\"# successes: {total_successes} ({total_successes / total_episodes * 100:.1f}%)\")\n",
    "            log_file.write(f\"Success: {done}\\n\")\n",
    "            log_file.write(f\"# episodes completed so far: {total_episodes}\\n\")\n",
    "            log_file.write(f\"# successes: {total_successes} ({total_successes / total_episodes * 100:.1f}%)\\n\")\n",
    "            log_file.flush()\n",
    "\n",
    "        # Log final results\n",
    "        print(f\"Current task success rate: {float(task_successes) / float(task_episodes)}\")\n",
    "        print(f\"Current total success rate: {float(total_successes) / float(total_episodes)}\")\n",
    "        log_file.write(f\"Current task success rate: {float(task_successes) / float(task_episodes)}\\n\")\n",
    "        log_file.write(f\"Current total success rate: {float(total_successes) / float(total_episodes)}\\n\")\n",
    "        log_file.flush()\n",
    "    # Save local log file\n",
    "    # log_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"a\": 1, \"b\": 2}\n",
    "\n",
    "def get(a, **kwargs):\n",
    "    print(a)\n",
    "    return a\n",
    "\n",
    "get(**inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
