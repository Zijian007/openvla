{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"/mnt/sda/home/zijianwang/HF_CACHE\"\n",
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import draccus\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import tqdm\n",
    "from accelerate import PartialState\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig\n",
    "from transformers import AutoConfig, AutoImageProcessor\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "import wandb\n",
    "from prismatic.models.backbones.llm.prompting import PurePromptBuilder, VicunaV15ChatPromptBuilder\n",
    "from prismatic.util.data_utils import PaddedCollatorForActionPrediction\n",
    "from prismatic.vla.action_tokenizer import ActionTokenizer\n",
    "from prismatic.vla.datasets import RLDSBatchTransform, RLDSDataset, EpisodicRLDSDataset\n",
    "from prismatic.vla.datasets.rlds.utils.data_utils import save_dataset_statistics\n",
    "\n",
    "from prismatic.extern.hf.configuration_prismatic import OpenVLAConfig\n",
    "from prismatic.extern.hf.modeling_prismatic import OpenVLAForActionPrediction\n",
    "from prismatic.extern.hf.processing_prismatic import PrismaticImageProcessor, PrismaticProcessor\n",
    "\n",
    "# Sane Defaults\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AutoConfig.register(\"openvla\", OpenVLAConfig)\n",
    "AutoImageProcessor.register(OpenVLAConfig, PrismaticImageProcessor)\n",
    "AutoProcessor.register(OpenVLAConfig, PrismaticProcessor)\n",
    "AutoModelForVision2Seq.register(OpenVLAConfig, OpenVLAForActionPrediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"openvla/openvla-7b\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vla = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"openvla/openvla-7b\", \n",
    "    attn_implementation=\"flash_attention_2\",  # [Optional] Requires `flash_attn`\n",
    "    torch_dtype=torch.bfloat16, \n",
    "    low_cpu_mem_usage=True, \n",
    "    trust_remote_code=True\n",
    ").to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_tokenizer = ActionTokenizer(processor.tokenizer)\n",
    "vocab_size = action_tokenizer.vocab_size\n",
    "print(\"词表大小:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vla_model_config = OpenVLAConfig.from_pretrained(\"openvla/openvla-7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vla_model_config.image_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_transform = RLDSBatchTransform(\n",
    "    action_tokenizer,\n",
    "    processor.tokenizer,\n",
    "    image_transform=processor.image_processor.apply_transform,\n",
    "    prompt_builder_fn=PurePromptBuilder if \"v01\" not in \"openvla/openvla-7b\" else VicunaV15ChatPromptBuilder,\n",
    ")\n",
    "\n",
    "vla_dataset = RLDSDataset(\n",
    "    \"/mnt/sda/home/zijianwang/openvla/modified_libero_rlds\",\n",
    "    \"libero_goal_no_noops\",\n",
    "    batch_transform,\n",
    "    resize_resolution=tuple(vla_model_config.image_sizes),\n",
    "    shuffle_buffer_size=100_000,\n",
    "    image_aug=True,\n",
    ")\n",
    "\n",
    "episodic_vla_dataset = EpisodicRLDSDataset(\n",
    "    \"/mnt/sda/home/zijianwang/openvla/modified_libero_rlds\",\n",
    "    \"libero_goal_no_noops\",\n",
    "    batch_transform,\n",
    "    resize_resolution=tuple(vla_model_config.image_sizes),\n",
    "    shuffle_buffer_size=100_000,\n",
    "    image_aug=False,\n",
    "    if_random_start=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio, os\n",
    "import numpy as np\n",
    "imgs = data[\"replay_images\"]\n",
    "mp4_path = \"test3.mp4\"\n",
    "# os.makedirs(os.path.dirname(mp4_path), exist_ok=True)\n",
    "video_writer = imageio.get_writer(mp4_path, fps=30)\n",
    "for img in imgs[:]:    \n",
    "    video_writer.append_data(img)\n",
    "video_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in vla_dataset:\n",
    "    print(data[\"input_ids\"].shape)\n",
    "    print(data[\"labels\"].shape)\n",
    "    print(data[\"pixel_values\"].shape)\n",
    "    print(data[\"dataset_name\"])\n",
    "    print(data.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"action\"])\n",
    "print(data[\"img\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = PaddedCollatorForActionPrediction(\n",
    "    processor.tokenizer.model_max_length, processor.tokenizer.pad_token_id, padding_side=\"right\"\n",
    ")\n",
    "dataloader = DataLoader(\n",
    "    vla_dataset,\n",
    "    batch_size=100,\n",
    "    sampler=None,\n",
    "    collate_fn=collator,\n",
    "    num_workers=0,  # Important =>> Set to 0 if using RLDS; TFDS rolls its own parallelism!\n",
    ")\n",
    "print(len(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm.tqdm(total=20000, leave=False) as progress:\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        progress.update()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = data\n",
    "device_id = vla.device\n",
    "output: CausalLMOutputWithPast = vla(\n",
    "    input_ids=batch[\"input_ids\"].to(device_id),\n",
    "    # attention_mask=batch[\"attention_mask\"].to(device_id),\n",
    "    pixel_values=batch[\"pixel_values\"].to(torch.bfloat16).to(device_id),\n",
    "    labels=batch[\"labels\"],\n",
    ")\n",
    "\n",
    "inputs = {\n",
    "    \"input_ids\": batch[\"input_ids\"].to(device_id),\n",
    "    \"pixel_values\": batch[\"pixel_values\"].to(torch.bfloat16).to(device_id),\n",
    "    \"labels\": batch[\"labels\"],\n",
    "}\n",
    "action = vla.predict_action(**inputs, unnorm_key=\"bridge_orig\", do_sample=False)\n",
    "\n",
    "# action_ids = vla.predict_action_ids(**inputs, unnorm_key=\"bridge_orig\", do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = batch\n",
    "print(data[\"input_ids\"][0].shape)\n",
    "print(processor.tokenizer.decode(data[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = data[\"pixel_values\"][0,0:3]  # Shape: [3, 224, 224]\n",
    "print(img.shape)\n",
    "\n",
    "# Convert from CHW to HWC format and scale to 0-255 range\n",
    "img = img.permute(1, 2, 0)  # Shape: [224, 224, 3]\n",
    "img = (img * 255).clamp(0, 255).to(torch.uint8)\n",
    "\n",
    "\n",
    "# 将tensor转换为numpy数组并调整通道顺序\n",
    "img_np = img.numpy()\n",
    "\n",
    "# 创建新的图形\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(img_np)\n",
    "plt.axis('off')  # 不显示坐标轴\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = data[\"pixel_values\"][0,3:6]  # Shape: [3, 224, 224]\n",
    "print(img.shape)\n",
    "\n",
    "# Convert from CHW to HWC format and scale to 0-255 range\n",
    "img = img.permute(1, 2, 0)  # Shape: [224, 224, 3]\n",
    "img = (img * 255).clamp(0, 255).to(torch.uint8)\n",
    "\n",
    "\n",
    "# 将tensor转换为numpy数组并调整通道顺序\n",
    "img_np = img.numpy()\n",
    "\n",
    "# 创建新的图形\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(img_np)\n",
    "plt.axis('off')  # 不显示坐标轴\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, pickle, os, random\n",
    "from experiments.robot.libero.libero_utils import (\n",
    "    get_libero_dummy_action,\n",
    "    get_libero_env,\n",
    "    get_libero_image)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_size = 224\n",
    "base_path = \"/mnt/sda/home/zijianwang/openvla/vla-scripts/DPO/winner_trajectory/libero_10\"\n",
    "\n",
    "# Find all folders with positive episode numbers\n",
    "trajectory_folders = []\n",
    "for folder_name in os.listdir(base_path):\n",
    "    match = re.search(r'task_(\\d+)_episode_(\\d+)_success', folder_name)\n",
    "    if match:\n",
    "        episode_num = int(match.group(2))\n",
    "        if episode_num > 0:  # Only positive episode numbers\n",
    "            trajectory_folders.append(folder_name)\n",
    "\n",
    "print(f\"Found {len(trajectory_folders)} trajectories with positive episode numbers\")\n",
    "\n",
    "# Process each trajectory folder\n",
    "for folder_name in trajectory_folders:\n",
    "    trajectory_folder_path = os.path.join(base_path, folder_name)\n",
    "    print(f\"Processing: {folder_name}\")\n",
    "    \n",
    "    pkl_files = [f for f in os.listdir(trajectory_folder_path) if f.endswith(\".pkl\")]\n",
    "    \n",
    "    # Sort pkl files by step number\n",
    "    pkl_files_sorted = []\n",
    "    for pkl_file in pkl_files:\n",
    "        match = re.search(r'step_(\\d+)\\.pkl', pkl_file)\n",
    "        if match:\n",
    "            pkl_files_sorted.append(pkl_file)\n",
    "    \n",
    "    pkl_files_sorted.sort(key=lambda x: int(re.search(r'step_(\\d+)\\.pkl', x).group(1)))\n",
    "    \n",
    "    start_idx = 0\n",
    "    action_sperate_token_id = 32001\n",
    "    imgs = []\n",
    "    \n",
    "    for i in range(start_idx, len(pkl_files_sorted)):\n",
    "        with open(os.path.join(trajectory_folder_path, pkl_files_sorted[i]), \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "            state = data[\"obs\"]\n",
    "            if type(state).__name__ == 'OrderedDict':\n",
    "                img = get_libero_image(state, resize_size) \n",
    "            elif type(state) == np.ndarray:\n",
    "                img = data[\"obs\"]\n",
    "            imgs.append(img)\n",
    "    \n",
    "    if len(imgs) > 0:\n",
    "        mp4_path = os.path.join(trajectory_folder_path, f\"Avideo_{len(imgs)}.mp4\")\n",
    "        video_writer = imageio.get_writer(mp4_path, fps=30)\n",
    "        for step, img in enumerate(imgs):    \n",
    "            video_writer.append_data(img)\n",
    "        video_writer.close()\n",
    "        print(f\"Saved video: {mp4_path}\")\n",
    "    else:\n",
    "        print(f\"No images found for {folder_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(state))\n",
    "print(img.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
