{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 17:58:24.257243: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-23 17:58:24.257297: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-23 17:58:24.259292: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-23 17:58:24.275526: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-23 17:58:25.235468: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mNo private macro file found! (macros.py:53)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mIt is recommended to use a private macro file (macros.py:54)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mTo setup, run: python /home/zijianwang/miniconda3/envs/openvla/lib/python3.10/site-packages/robosuite/scripts/setup_macros.py (macros.py:55)\n",
      "/home/zijianwang/miniconda3/envs/openvla/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union\n",
    "\n",
    "import draccus\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from libero.libero import benchmark\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Append current directory so that interpreter can find experiments.robot\n",
    "sys.path.append(\"../..\")\n",
    "from experiments.robot.libero.libero_utils import (\n",
    "    get_libero_dummy_action,\n",
    "    get_libero_env,\n",
    "    get_libero_image,\n",
    "    quat2axisangle,\n",
    "    save_rollout_video,\n",
    ")\n",
    "from experiments.robot.openvla_utils import get_processor\n",
    "from experiments.robot.robot_utils import (\n",
    "    DATE_TIME,\n",
    "    get_action,\n",
    "    get_image_resize_size,\n",
    "    get_model,\n",
    "    get_vla_via_lora,\n",
    "    invert_gripper_action,\n",
    "    normalize_gripper_action,\n",
    "    set_seed_everywhere,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GenerateConfig:\n",
    "    # fmt: off\n",
    "\n",
    "    #################################################################################################################\n",
    "    # Model-specific parameters\n",
    "    #################################################################################################################\n",
    "    model_family: str = \"openvla\"                    # Model family\n",
    "    pretrained_checkpoint: Union[str, Path] = \"/mnt/sda/home/zijianwang/openvla/FT_res/openvla-7b-finetuned-libero-10+libero_10_no_noops+b4+lr-0.0005+lora-r48+dropout-0.0--image_aug--2025-07-18_19-26-25\"     # Pretrained checkpoint path\n",
    "\n",
    "    lora_path: str = \"/mnt/sda/home/zijianwang/openvla/adapter_tmp_dir/openvla-7b-finetuned-libero-10+libero_10_no_noops+b4+lr-0.0005+lora-r48+dropout-0.0--image_aug--2025-07-18_19-26-25\"\n",
    "    load_in_8bit: bool = False                       # (For OpenVLA only) Load with 8-bit quantization\n",
    "    load_in_4bit: bool = False                       # (For OpenVLA only) Load with 4-bit quantization\n",
    "\n",
    "    center_crop: bool = True                         # Center crop? (if trained w/ random crop image aug)\n",
    "\n",
    "    #################################################################################################################\n",
    "    # LIBERO environment-specific parameters\n",
    "    #################################################################################################################\n",
    "    task_suite_name: str = \"libero_10\"          # Task suite. Options: libero_spatial, libero_object, libero_goal, libero_10, libero_90\n",
    "    num_steps_wait: int = 10                         # Number of steps to wait for objects to stabilize in sim\n",
    "    num_trials_per_task: int = 50                    # Number of rollouts per task\n",
    "\n",
    "    #################################################################################################################\n",
    "    # Utils\n",
    "    #################################################################################################################\n",
    "    run_id_note: Optional[str] = None                # Extra note to add in run ID for logging\n",
    "    local_log_dir: str = \"./experiments/logs\"        # Local directory for eval logs\n",
    "\n",
    "    use_wandb: bool = False                          # Whether to also log results in Weights & Biases\n",
    "    wandb_project: str = \"YOUR_WANDB_PROJECT\"        # Name of W&B project to log to (use default!)\n",
    "    wandb_entity: str = \"YOUR_WANDB_ENTITY\"          # Name of entity to log under\n",
    "\n",
    "    seed: int = 7                                    # Random Seed (for reproducibility)\n",
    "\n",
    "    device: str = \"cuda:0\"\n",
    "\n",
    "    # fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model with Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoConfig, AutoImageProcessor, AutoModelForVision2Seq, AutoProcessor\n",
    "from prismatic.extern.hf.configuration_prismatic import OpenVLAConfig\n",
    "from prismatic.extern.hf.modeling_prismatic import OpenVLAForActionPrediction\n",
    "from prismatic.extern.hf.processing_prismatic import PrismaticImageProcessor, PrismaticProcessor\n",
    "import torch\n",
    "AutoConfig.register(\"openvla\", OpenVLAConfig)\n",
    "AutoImageProcessor.register(OpenVLAConfig, PrismaticImageProcessor)\n",
    "AutoProcessor.register(OpenVLAConfig, PrismaticProcessor)\n",
    "AutoModelForVision2Seq.register(OpenVLAConfig, OpenVLAForActionPrediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_path = \"/mnt/sda/home/zijianwang/openvla/adapter_tmp_dir/openvla-7b-finetuned-libero-10+libero_10_no_noops+b4+lr-0.0005+lora-r48+dropout-0.0--image_aug--2025-07-18_19-26-25\"\n",
    "lora_config = PeftConfig.from_pretrained(lora_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.71s/it]\n"
     ]
    }
   ],
   "source": [
    "base_vla_path = lora_config.base_model_name_or_path\n",
    "base_vla = AutoModelForVision2Seq.from_pretrained(\n",
    "        base_vla_path,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        load_in_8bit=False,\n",
    "        load_in_4bit=False,\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"cuda:2\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(base_vla, lora_path, is_trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 166,242,432 || all params: 7,707,479,616 || trainable%: 2.1569\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "dataset_statistics_path = os.path.join(lora_config.base_model_name_or_path, \"dataset_statistics.json\")\n",
    "if os.path.isfile(dataset_statistics_path):\n",
    "    with open(dataset_statistics_path, \"r\") as f:\n",
    "        norm_stats = json.load(f)\n",
    "    model.norm_stats = norm_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Instantiating Pretrained VLA model\n",
      "[*] Loading in BF16 with Flash-Attention Enabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.36it/s]\n"
     ]
    }
   ],
   "source": [
    "cfg = GenerateConfig()\n",
    "assert cfg.pretrained_checkpoint is not None, \"cfg.pretrained_checkpoint must not be None!\"\n",
    "if \"image_aug\" in cfg.pretrained_checkpoint:\n",
    "    assert cfg.center_crop, \"Expecting `center_crop==True` because model was trained with image augmentations!\"\n",
    "assert not (cfg.load_in_8bit and cfg.load_in_4bit), \"Cannot use both 8-bit and 4-bit quantization!\"\n",
    "\n",
    "# Set random seed\n",
    "set_seed_everywhere(cfg.seed)\n",
    "\n",
    "# [OpenVLA] Set action un-normalization key\n",
    "cfg.unnorm_key = cfg.task_suite_name\n",
    "\n",
    "# Load model\n",
    "# model = get_model(cfg)\n",
    "\n",
    "model = get_vla_via_lora(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OpenVLAForActionPrediction' object has no attribute 'merge_and_unload'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_and_unload\u001b[49m()\n",
      "File \u001b[0;32m~/miniconda3/envs/openvla/lib/python3.10/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'OpenVLAForActionPrediction' object has no attribute 'merge_and_unload'"
     ]
    }
   ],
   "source": [
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to local log file: ./experiments/logs/EVAL-libero_10-openvla-2025_07_23-17_58_46.txt\n",
      "[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Task suite: libero_10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task ID: 6\n",
      "[Warning]: datasets path /mnt/sda/home/zijianwang/LIBERO/libero/libero/../datasets does not exist!\n",
      "[Warning]: datasets path /mnt/sda/home/zijianwang/LIBERO/libero/libero/../datasets does not exist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting task: 6, episode: 22...\n",
      "\n",
      "Task: put the white mug on the plate and put the chocolate pudding to the right of the plate\n",
      "t: 529\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 50/50 [04:29<00:00,  5.38s/it]\n",
      "100%|██████████| 10/10 [04:34<00:00, 27.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved rollout MP4 at path ./rollouts/2025_07_23/2025_07_23-17_58_46--episode=1--success=False--task=put_the_white_mug_on_the_plate_and_put_the_chocola.mp4\n",
      "Success: False\n",
      "# episodes completed so far: 1\n",
      "# successes: 0 (0.0%)\n",
      "Current task success rate: 0.0\n",
      "Current total success rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# [OpenVLA] Check that the model contains the action un-normalization key\n",
    "if cfg.model_family == \"openvla\":\n",
    "    # In some cases, the key must be manually modified (e.g. after training on a modified version of the dataset\n",
    "    # with the suffix \"_no_noops\" in the dataset name)\n",
    "    if cfg.unnorm_key not in model.norm_stats and f\"{cfg.unnorm_key}_no_noops\" in model.norm_stats:\n",
    "        cfg.unnorm_key = f\"{cfg.unnorm_key}_no_noops\"\n",
    "    assert cfg.unnorm_key in model.norm_stats, f\"Action un-norm key {cfg.unnorm_key} not found in VLA `norm_stats`!\"\n",
    "\n",
    "# [OpenVLA] Get Hugging Face processor\n",
    "processor = None\n",
    "if cfg.model_family == \"openvla\":\n",
    "    processor = get_processor(cfg)\n",
    "\n",
    "# Initialize local logging\n",
    "run_id = f\"EVAL-{cfg.task_suite_name}-{cfg.model_family}-{DATE_TIME}\"\n",
    "if cfg.run_id_note is not None:\n",
    "    run_id += f\"--{cfg.run_id_note}\"\n",
    "os.makedirs(cfg.local_log_dir, exist_ok=True)\n",
    "local_log_filepath = os.path.join(cfg.local_log_dir, run_id + \".txt\")\n",
    "log_file = open(local_log_filepath, \"w\")\n",
    "print(f\"Logging to local log file: {local_log_filepath}\")\n",
    "\n",
    "# Initialize Weights & Biases logging as well\n",
    "if cfg.use_wandb:\n",
    "    wandb.init(\n",
    "        entity=cfg.wandb_entity,\n",
    "        project=cfg.wandb_project,\n",
    "        name=run_id,\n",
    "    )\n",
    "\n",
    "# Initialize LIBERO task suite\n",
    "benchmark_dict = benchmark.get_benchmark_dict()\n",
    "task_suite = benchmark_dict[cfg.task_suite_name]()\n",
    "num_tasks_in_suite = task_suite.n_tasks\n",
    "print(f\"Task suite: {cfg.task_suite_name}\")\n",
    "log_file.write(f\"Task suite: {cfg.task_suite_name}\\n\")\n",
    "\n",
    "# Get expected image dimensions\n",
    "resize_size = get_image_resize_size(cfg)\n",
    "\n",
    "# Start evaluation\n",
    "total_episodes, total_successes = 0, 0\n",
    "for task_id in tqdm.tqdm(range(num_tasks_in_suite)):\n",
    "    if task_id != 6:\n",
    "        continue\n",
    "    # Get task\n",
    "    print(f\"Task ID: {task_id}\")\n",
    "    task = task_suite.get_task(task_id)\n",
    "\n",
    "    # Get default LIBERO initial states\n",
    "    initial_states = task_suite.get_task_init_states(task_id)\n",
    "\n",
    "    # Initialize LIBERO environment and task description\n",
    "    env, task_description = get_libero_env(task, cfg.model_family, resolution=256)\n",
    "\n",
    "    # Start episodes\n",
    "    task_episodes, task_successes = 0, 0\n",
    "    for episode_idx in tqdm.tqdm(range(cfg.num_trials_per_task)):\n",
    "        if episode_idx != 22:\n",
    "            continue\n",
    "        print(f\"Starting task: {task_id}, episode: {episode_idx}...\")\n",
    "        log_file.write(f\"Starting task: {task_id}, episode: {episode_idx}...\\n\")\n",
    "\n",
    "        print(f\"\\nTask: {task_description}\")\n",
    "        log_file.write(f\"\\nTask: {task_description}\\n\")\n",
    "\n",
    "        # Reset environment\n",
    "        env.reset()\n",
    "\n",
    "        # Set initial states\n",
    "        obs = env.set_init_state(initial_states[episode_idx])\n",
    "\n",
    "        # Setup\n",
    "        t = 0\n",
    "        replay_images = []\n",
    "        if cfg.task_suite_name == \"libero_spatial\":\n",
    "            max_steps = 220  # longest training demo has 193 steps\n",
    "        elif cfg.task_suite_name == \"libero_object\":\n",
    "            max_steps = 280  # longest training demo has 254 steps\n",
    "        elif cfg.task_suite_name == \"libero_goal\":\n",
    "            max_steps = 300  # longest training demo has 270 steps\n",
    "        elif cfg.task_suite_name == \"libero_10\":\n",
    "            max_steps = 520  # longest training demo has 505 steps\n",
    "        elif cfg.task_suite_name == \"libero_90\":\n",
    "            max_steps = 400  # longest training demo has 373 steps\n",
    "\n",
    "        # print(f\"Starting episode {task_episodes+1}...\")\n",
    "        # log_file.write(f\"Starting episode {task_episodes+1}...\\n\")\n",
    "        while t < max_steps + cfg.num_steps_wait:\n",
    "            print(f\"t: {t}\", end=\"\\r\")\n",
    "            try:\n",
    "                # IMPORTANT: Do nothing for the first few timesteps because the simulator drops objects\n",
    "                # and we need to wait for them to fall\n",
    "                if t < cfg.num_steps_wait:\n",
    "                    obs, reward, done, info = env.step(get_libero_dummy_action(cfg.model_family))\n",
    "                    t += 1\n",
    "                    continue\n",
    "                # obs: OrderedDict[str, np.ndarray]\n",
    "\n",
    "                # Get preprocessed image\n",
    "                img = get_libero_image(obs, resize_size)\n",
    "\n",
    "                # Save preprocessed image for replay video\n",
    "                replay_images.append(img)\n",
    "\n",
    "                # Prepare observations dict\n",
    "                # Note: OpenVLA does not take proprio state as input\n",
    "                observation = {\n",
    "                    \"full_image\": img,\n",
    "                    \"state\": np.concatenate(\n",
    "                        (obs[\"robot0_eef_pos\"], quat2axisangle(obs[\"robot0_eef_quat\"]), obs[\"robot0_gripper_qpos\"])\n",
    "                    ),\n",
    "                }\n",
    "\n",
    "                # Query model to get action\n",
    "                action = get_action(\n",
    "                    cfg,\n",
    "                    model,\n",
    "                    observation,\n",
    "                    task_description,\n",
    "                    processor=processor,\n",
    "                )\n",
    "\n",
    "                # Normalize gripper action [0,1] -> [-1,+1] because the environment expects the latter\n",
    "                action = normalize_gripper_action(action, binarize=True)\n",
    "\n",
    "                # [OpenVLA] The dataloader flips the sign of the gripper action to align with other datasets\n",
    "                # (0 = close, 1 = open), so flip it back (-1 = open, +1 = close) before executing the action\n",
    "                if cfg.model_family == \"openvla\":\n",
    "                    action = invert_gripper_action(action)\n",
    "\n",
    "                # Execute action in environment\n",
    "                obs, reward, done, info = env.step(action.tolist())\n",
    "                if done:\n",
    "                    task_successes += 1\n",
    "                    total_successes += 1\n",
    "                    break\n",
    "                t += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Caught exception: {e}\")\n",
    "                log_file.write(f\"Caught exception: {e}\\n\")\n",
    "                break\n",
    "\n",
    "        task_episodes += 1\n",
    "        total_episodes += 1\n",
    "\n",
    "        # Save a replay video of the episode\n",
    "        save_rollout_video(\n",
    "            replay_images, total_episodes, success=done, task_description=task_description, log_file=log_file\n",
    "        )\n",
    "\n",
    "        # Log current results\n",
    "        print(f\"Success: {done}\")\n",
    "        print(f\"# episodes completed so far: {total_episodes}\")\n",
    "        print(f\"# successes: {total_successes} ({total_successes / total_episodes * 100:.1f}%)\")\n",
    "        log_file.write(f\"Success: {done}\\n\")\n",
    "        log_file.write(f\"# episodes completed so far: {total_episodes}\\n\")\n",
    "        log_file.write(f\"# successes: {total_successes} ({total_successes / total_episodes * 100:.1f}%)\\n\")\n",
    "        log_file.flush()\n",
    "\n",
    "    # Log final results\n",
    "    print(f\"Current task success rate: {float(task_successes) / float(task_episodes)}\")\n",
    "    print(f\"Current total success rate: {float(total_successes) / float(total_episodes)}\")\n",
    "    log_file.write(f\"Current task success rate: {float(task_successes) / float(task_episodes)}\\n\")\n",
    "    log_file.write(f\"Current total success rate: {float(total_successes) / float(total_episodes)}\\n\")\n",
    "    log_file.flush()\n",
    "    if cfg.use_wandb:\n",
    "        wandb.log(\n",
    "            {\n",
    "                f\"success_rate/{task_description}\": float(task_successes) / float(task_episodes),\n",
    "                f\"num_episodes/{task_description}\": task_episodes,\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Save local log file\n",
    "log_file.close()\n",
    "\n",
    "# Push total metrics and local log file to wandb\n",
    "if cfg.use_wandb:\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"success_rate/total\": float(total_successes) / float(total_episodes),\n",
    "            \"num_episodes/total\": total_episodes,\n",
    "        }\n",
    "    )\n",
    "    wandb.save(local_log_filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
