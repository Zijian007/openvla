{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# OpenVLA DPO Training Demo - Professional Version\n",
        "\n",
        "This notebook provides a modular implementation of Direct Preference Optimization (DPO) training for OpenVLA models. Each section can be run independently for debugging and testing purposes.\n",
        "\n",
        "## Overview\n",
        "1. **Environment Setup** - Import libraries and configure paths\n",
        "2. **Configuration** - Set training parameters and model configs\n",
        "3. **Model Loading** - Load policy and reference models\n",
        "4. **Data Loading** - Setup datasets and data loaders\n",
        "5. **DPO Training** - Main training loop\n",
        "6. **Testing & Debugging** - Utilities for debugging each component\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup and Imports\n",
        "\n",
        "Import all necessary libraries and setup the Python path for accessing local modules.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TO DO\n",
        "## 1. 修改计算logprob时的mask, 不算separate action token的loss.\n",
        "## 2. 对一个stream中每个action units赋予不同的weights, according to spatial distance.\n",
        "## 3. 同时具备离线和在线的loser 轨迹采集\n",
        "## 4. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2           "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added to Python path: /mnt/sda/home/zijianwang/openvla/vla-scripts/DPO/../..\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-30 20:02:44.233356: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-08-30 20:02:44.233389: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-08-30 20:02:44.234889: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-08-30 20:02:44.243708: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-08-30 20:02:45.715621: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mNo private macro file found! (macros.py:53)\n",
            "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mIt is recommended to use a private macro file (macros.py:54)\n",
            "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mTo setup, run: python /home/zijianwang/miniconda3/envs/openvla/lib/python3.10/site-packages/robosuite/scripts/setup_macros.py (macros.py:55)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Successfully imported local modules\n",
            "✓ Successfully imported external modules\n",
            "\n",
            "GPU Information:\n",
            "CUDA available: True\n",
            "GPU count: 4\n",
            "  GPU 0: NVIDIA GeForce RTX 4090\n",
            "    Memory: 25.4 GB\n",
            "  GPU 1: NVIDIA GeForce RTX 4090\n",
            "    Memory: 25.4 GB\n",
            "  GPU 2: NVIDIA GeForce RTX 4090\n",
            "    Memory: 25.4 GB\n",
            "  GPU 3: NVIDIA RTX A6000\n",
            "    Memory: 51.0 GB\n",
            "\n",
            "==================================================\n",
            "Environment setup completed!\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "DPO Training Demo - Environment Setup\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add the parent directories to Python path for imports\n",
        "current_dir = os.getcwd()\n",
        "parent_dir = os.path.join(current_dir, \"..\", \"..\")\n",
        "sys.path.append(parent_dir)\n",
        "print(f\"Added to Python path: {parent_dir}\")\n",
        "\n",
        "# Core imports\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from trl.trainer.dpo_trainer import DataCollatorForPreference\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from experiments.robot.libero.libero_utils import (\n",
        "    get_libero_dummy_action,\n",
        "    get_libero_env,\n",
        "    get_libero_image,\n",
        "    quat2axisangle,\n",
        "    save_rollout_video_CoA,\n",
        ")\n",
        "\n",
        "# Local imports\n",
        "try:\n",
        "    from src.config import GenerateConfig\n",
        "    from src.model_utils import setup_vla_model_with_lora, setup_model_and_config, setup_logging_and_environment\n",
        "    from src.training_utils_prog import train_dpo, compute_log_probs, dpo_loss, grouped_dpo_loss\n",
        "    from src.data_process import TrajectoryDataset\n",
        "    print(\"✓ Successfully imported local modules\")\n",
        "except ImportError as e:\n",
        "    print(f\"✗ Failed to import local modules: {e}\")\n",
        "    print(\"Please ensure you're running from the correct directory\")\n",
        "\n",
        "# External imports  \n",
        "try:\n",
        "    from experiments.robot.robot_utils import get_model\n",
        "    print(\"✓ Successfully imported external modules\")\n",
        "except ImportError as e:\n",
        "    print(f\"✗ Failed to import external modules: {e}\")\n",
        "\n",
        "# Check GPU availability\n",
        "print(f\"\\nGPU Information:\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "        print(f\"    Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Environment setup completed!\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration Setup\n",
        "\n",
        "Configure all training parameters. You can modify these parameters easily for different experiments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating configuration...\n",
            "\n",
            "==================================================\n",
            "CONFIGURATION SUMMARY\n",
            "==================================================\n",
            "Policy Device: cuda:1\n",
            "Reference Device: cuda:2\n",
            "Max Steps: 800\n",
            "Batch Size: 1\n",
            "Learning Rate: 0.0001\n",
            "DPO Beta: 0.5\n",
            "Stream Length: 5\n",
            "Use WandB: False\n",
            "Task Number: 1\n",
            "\n",
            "Path Configuration:\n",
            "Root Dir: /mnt/sda/home/zijianwang\n",
            "Pretrained Checkpoint: /mnt/sda/home/zijianwang/openvla/FT_res/openvla-7b-finetuned-libero-10+libero_10_no_noops+b4+lr-0.0005+lora-r48+dropout-0.0--image_aug--2025-07-18_19-26-25\n",
            "LoRA Path: /mnt/sda/home/zijianwang/openvla/adapter_tmp_dir/openvla-7b-finetuned-libero-10+libero_10_no_noops+b4+lr-0.0005+lora-r48+dropout-0.0--image_aug--2025-07-18_19-26-25\n",
            "Winner Trajectory Path: /mnt/sda/home/zijianwang/openvla/vla-scripts/DPO/winner_trajectory\n",
            "Adapter Tmp Dir: /mnt/sda/home/zijianwang/openvla/DPO_adapter_tmp_dir\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Configuration Setup - Modify parameters here for different experiments\n",
        "\"\"\"\n",
        "\n",
        "# ====== TRAINING PARAMETERS ======\n",
        "DEVICE_POLICY = \"cuda:1\"  # Device for policy model\n",
        "DEVICE_REF = \"cuda:2\"     # Device for reference model\n",
        "MAX_STEPS = 800           # Maximum training steps (reduced for demo)\n",
        "BATCH_SIZE = 1            # Training batch size\n",
        "LEARNING_RATE = 0.0001    # Learning rate\n",
        "DPO_BETA = 0.5           # DPO beta parameter\n",
        "STREAM_LENGTH = 5       # Stream length for trajectory processing\n",
        "\n",
        "# ====== WANDB CONFIGURATION ======\n",
        "USE_WANDB = False         # Set to True to enable Weights & Biases logging\n",
        "WANDB_PROJECT = \"openvla_CoA_DPO_demo\"\n",
        "WANDB_ENTITY = \"15652388600\"\n",
        "RUN_ID_NOTE = \"notebook_demo\"\n",
        "\n",
        "# ====== PATH CONFIGURATION ======\n",
        "ROOT_DIR = \"/mnt/sda/home/zijianwang\"\n",
        "\n",
        "# Optional: Override default paths (leave empty to use defaults)\n",
        "PRETRAINED_CHECKPOINT = f\"{ROOT_DIR}/openvla/FT_res/openvla-7b-finetuned-libero-10+libero_10_no_noops+b4+lr-0.0005+lora-r48+dropout-0.0--image_aug--2025-07-18_19-26-25\"\n",
        "LORA_PATH = f\"{ROOT_DIR}/openvla/adapter_tmp_dir/openvla-7b-finetuned-libero-10+libero_10_no_noops+b4+lr-0.0005+lora-r48+dropout-0.0--image_aug--2025-07-18_19-26-25\"\n",
        "BASE_VLA_PATH = f\"{ROOT_DIR}/HF_CACHE/openvla-7b-finetuned-libero-10\"\n",
        "WINNER_TRAJECTORY_PATH = f\"{ROOT_DIR}/openvla/vla-scripts/DPO/winner_trajectory\"\n",
        "ADAPTER_TMP_DIR = f\"{ROOT_DIR}/openvla/DPO_adapter_tmp_dir\"\n",
        "TASK_NUM = 1            # Set to specific task number or None for all tasks\n",
        "# Create configuration objects\n",
        "print(\"Creating configuration...\")\n",
        "\n",
        "# Policy model configuration\n",
        "model_cfg = GenerateConfig(\n",
        "    root_dir=ROOT_DIR,\n",
        "    device=DEVICE_POLICY,\n",
        "    max_steps=MAX_STEPS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    dpo_beta=DPO_BETA,\n",
        "    stream_length=STREAM_LENGTH,\n",
        "    use_wandb=USE_WANDB,\n",
        "    wandb_project=WANDB_PROJECT,\n",
        "    wandb_entity=WANDB_ENTITY,\n",
        "    run_id_note=RUN_ID_NOTE,\n",
        "    grad_accumulation_steps=1,\n",
        "    pretrained_checkpoint=PRETRAINED_CHECKPOINT,\n",
        "    lora_path=LORA_PATH,\n",
        "    base_vla_path=BASE_VLA_PATH,\n",
        "    winner_trajectory_path=WINNER_TRAJECTORY_PATH,\n",
        "    adapter_tmp_dir=ADAPTER_TMP_DIR,\n",
        "    task_num=TASK_NUM\n",
        ")\n",
        "\n",
        "# Reference model configuration\n",
        "ref_config = GenerateConfig(\n",
        "    root_dir=ROOT_DIR,\n",
        "    device=DEVICE_REF,\n",
        "    pretrained_checkpoint=PRETRAINED_CHECKPOINT,\n",
        "    lora_path=LORA_PATH,\n",
        "    base_vla_path=BASE_VLA_PATH,\n",
        "    winner_trajectory_path=WINNER_TRAJECTORY_PATH,\n",
        "    adapter_tmp_dir=ADAPTER_TMP_DIR\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"CONFIGURATION SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Policy Device: {model_cfg.device}\")\n",
        "print(f\"Reference Device: {ref_config.device}\")\n",
        "print(f\"Max Steps: {model_cfg.max_steps}\")\n",
        "print(f\"Batch Size: {model_cfg.batch_size}\")\n",
        "print(f\"Learning Rate: {model_cfg.learning_rate}\")\n",
        "print(f\"DPO Beta: {model_cfg.dpo_beta}\")\n",
        "print(f\"Stream Length: {model_cfg.stream_length}\")\n",
        "print(f\"Use WandB: {model_cfg.use_wandb}\")\n",
        "print(f\"Task Number: {model_cfg.task_num if model_cfg.task_num else 'All tasks'}\")\n",
        "print(\"\\nPath Configuration:\")\n",
        "print(f\"Root Dir: {model_cfg.root_dir}\")\n",
        "print(f\"Pretrained Checkpoint: {model_cfg.pretrained_checkpoint}\")\n",
        "print(f\"LoRA Path: {model_cfg.lora_path}\")\n",
        "print(f\"Winner Trajectory Path: {model_cfg.winner_trajectory_path}\")\n",
        "print(f\"Adapter Tmp Dir: {model_cfg.adapter_tmp_dir}\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Loading\n",
        "\n",
        "Load the policy model (with LoRA) and reference model. This section handles device placement and model initialization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting model loading...\n",
            "This may take several minutes depending on model size and device speed.\n",
            "\n",
            "------------------------------\n",
            "[1/2] Loading policy model (with LoRA)...\n",
            "Target device: cuda:1\n",
            "[*] Instantiating Pretrained VLA model\n",
            "[*] Loading in BF16 with Flash-Attention Enabled\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">08/30 [20:03:59] </span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> | &gt;&gt; Expected `<span style=\"color: #808000; text-decoration-color: #808000\">transformers</span>==<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.40</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>` and `<span style=\"color: #808000; text-decoration-color: #808000\">tokenizers</span>==<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.19</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>`   <a href=\"file:///home/zijianwang/.cache/huggingface/modules/transformers_modules/openvla/openvla-7b/31f090d05236101ebfc381b61c674dd4746d4ce0/modeling_prismatic.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">modeling_prismatic.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/zijianwang/.cache/huggingface/modules/transformers_modules/openvla/openvla-7b/31f090d05236101ebfc381b61c674dd4746d4ce0/modeling_prismatic.py#228\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">228</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         but got `<span style=\"color: #808000; text-decoration-color: #808000\">transformers</span>==<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.53</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>` and `<span style=\"color: #808000; text-decoration-color: #808000\">tokenizers</span>==<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.21</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>`; there  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                         </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         might be inference-time regressions due to dependency changes.  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                         </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         If in doubt, pleaseuse the above versions.                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                         </span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[2;36m08/30 [20:03:59]\u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m | >> Expected `\u001b[33mtransformers\u001b[0m==\u001b[1;36m4.40\u001b[0m.\u001b[1;36m1\u001b[0m` and `\u001b[33mtokenizers\u001b[0m==\u001b[1;36m0.19\u001b[0m.\u001b[1;36m1\u001b[0m`   \u001b]8;id=75954;file:///home/zijianwang/.cache/huggingface/modules/transformers_modules/openvla/openvla-7b/31f090d05236101ebfc381b61c674dd4746d4ce0/modeling_prismatic.py\u001b\\\u001b[2mmodeling_prismatic.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=861168;file:///home/zijianwang/.cache/huggingface/modules/transformers_modules/openvla/openvla-7b/31f090d05236101ebfc381b61c674dd4746d4ce0/modeling_prismatic.py#228\u001b\\\u001b[2m228\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                 \u001b[0m         but got `\u001b[33mtransformers\u001b[0m==\u001b[1;36m4.53\u001b[0m.\u001b[1;36m3\u001b[0m` and `\u001b[33mtokenizers\u001b[0m==\u001b[1;36m0.21\u001b[0m.\u001b[1;36m2\u001b[0m`; there  \u001b[2m                         \u001b[0m\n",
              "\u001b[2;36m                 \u001b[0m         might be inference-time regressions due to dependency changes.  \u001b[2m                         \u001b[0m\n",
              "\u001b[2;36m                 \u001b[0m         If in doubt, pleaseuse the above versions.                      \u001b[2m                         \u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 10.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Policy model loaded successfully\n",
            "Model device: cuda:1\n",
            "Model dtype: torch.bfloat16\n",
            "Total parameters: 7,707,479,616\n",
            "Trainable parameters: 166,242,432\n",
            "Trainable ratio: 2.16%\n",
            "\n",
            "------------------------------\n",
            "[2/2] Loading reference model...\n",
            "Target device: cuda:2\n",
            "[*] Instantiating Pretrained VLA model\n",
            "[*] Loading in BF16 with Flash-Attention Enabled\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">08/30 [20:04:09] </span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> | &gt;&gt; Expected `<span style=\"color: #808000; text-decoration-color: #808000\">transformers</span>==<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.40</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>` and `<span style=\"color: #808000; text-decoration-color: #808000\">tokenizers</span>==<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.19</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>`   <a href=\"file:///home/zijianwang/.cache/huggingface/modules/transformers_modules/openvla/openvla-7b/31f090d05236101ebfc381b61c674dd4746d4ce0/modeling_prismatic.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">modeling_prismatic.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/zijianwang/.cache/huggingface/modules/transformers_modules/openvla/openvla-7b/31f090d05236101ebfc381b61c674dd4746d4ce0/modeling_prismatic.py#228\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">228</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         but got `<span style=\"color: #808000; text-decoration-color: #808000\">transformers</span>==<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.53</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>` and `<span style=\"color: #808000; text-decoration-color: #808000\">tokenizers</span>==<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.21</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>`; there  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                         </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         might be inference-time regressions due to dependency changes.  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                         </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         If in doubt, pleaseuse the above versions.                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                         </span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[2;36m08/30 [20:04:09]\u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m | >> Expected `\u001b[33mtransformers\u001b[0m==\u001b[1;36m4.40\u001b[0m.\u001b[1;36m1\u001b[0m` and `\u001b[33mtokenizers\u001b[0m==\u001b[1;36m0.19\u001b[0m.\u001b[1;36m1\u001b[0m`   \u001b]8;id=682554;file:///home/zijianwang/.cache/huggingface/modules/transformers_modules/openvla/openvla-7b/31f090d05236101ebfc381b61c674dd4746d4ce0/modeling_prismatic.py\u001b\\\u001b[2mmodeling_prismatic.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=50631;file:///home/zijianwang/.cache/huggingface/modules/transformers_modules/openvla/openvla-7b/31f090d05236101ebfc381b61c674dd4746d4ce0/modeling_prismatic.py#228\u001b\\\u001b[2m228\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                 \u001b[0m         but got `\u001b[33mtransformers\u001b[0m==\u001b[1;36m4.53\u001b[0m.\u001b[1;36m3\u001b[0m` and `\u001b[33mtokenizers\u001b[0m==\u001b[1;36m0.21\u001b[0m.\u001b[1;36m2\u001b[0m`; there  \u001b[2m                         \u001b[0m\n",
              "\u001b[2;36m                 \u001b[0m         might be inference-time regressions due to dependency changes.  \u001b[2m                         \u001b[0m\n",
              "\u001b[2;36m                 \u001b[0m         If in doubt, pleaseuse the above versions.                      \u001b[2m                         \u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 10.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Reference model loaded successfully\n",
            "Model device: cuda:2\n",
            "Model dtype: torch.bfloat16\n",
            "✓ Reference model set to eval mode and frozen\n",
            "\n",
            "==================================================\n",
            "MODEL LOADING SUMMARY\n",
            "==================================================\n",
            "Policy Model Device: cuda:1\n",
            "Reference Model Device: cuda:2\n",
            "Policy Model Trainable: 878 params\n",
            "Reference Model Trainable: 0 params\n",
            "Models loaded successfully!\n",
            "==================================================\n",
            "GPU cache cleared.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Model Loading Section\n",
        "\"\"\"\n",
        "\n",
        "print(\"Starting model loading...\")\n",
        "print(\"This may take several minutes depending on model size and device speed.\")\n",
        "print(\"\\n\" + \"-\"*30)\n",
        "\n",
        "# Load policy model with LoRA\n",
        "print(\"[1/2] Loading policy model (with LoRA)...\")\n",
        "print(f\"Target device: {model_cfg.device}\")\n",
        "\n",
        "try:\n",
        "    policy_model = setup_vla_model_with_lora(model_cfg)\n",
        "    print(f\"✓ Policy model loaded successfully\")\n",
        "    print(f\"Model device: {next(policy_model.parameters()).device}\")\n",
        "    print(f\"Model dtype: {next(policy_model.parameters()).dtype}\")\n",
        "    \n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in policy_model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in policy_model.parameters() if p.requires_grad)\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "    print(f\"Trainable ratio: {100 * trainable_params / total_params:.2f}%\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"✗ Failed to load policy model: {e}\")\n",
        "    raise\n",
        "\n",
        "print(\"\\n\" + \"-\"*30)\n",
        "\n",
        "# Load reference model\n",
        "print(\"[2/2] Loading reference model...\")\n",
        "print(f\"Target device: {ref_config.device}\")\n",
        "\n",
        "try:\n",
        "    ref_model = setup_vla_model_with_lora(ref_config)\n",
        "    print(f\"✓ Reference model loaded successfully\")\n",
        "    print(f\"Model device: {next(ref_model.parameters()).device}\")\n",
        "    print(f\"Model dtype: {next(ref_model.parameters()).dtype}\")\n",
        "    \n",
        "    # Set reference model to eval mode and freeze parameters\n",
        "    ref_model.eval()\n",
        "    for param in ref_model.parameters():\n",
        "        param.requires_grad = False\n",
        "    print(\"✓ Reference model set to eval mode and frozen\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"✗ Failed to load reference model: {e}\")\n",
        "    raise\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MODEL LOADING SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Policy Model Device: {next(policy_model.parameters()).device}\")\n",
        "print(f\"Reference Model Device: {next(ref_model.parameters()).device}\")\n",
        "print(f\"Policy Model Trainable: {sum(p.requires_grad for p in policy_model.parameters())} params\")\n",
        "print(f\"Reference Model Trainable: {sum(p.requires_grad for p in ref_model.parameters())} params\")\n",
        "print(\"Models loaded successfully!\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Optional: Clear cache to free up memory\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"GPU cache cleared.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logging to local log file: ./experiments/logs/DPO-libero_10-openvla-2025_08_30-20_03_07--notebook_demo.txt\n",
            "[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Task suite: libero_10\n"
          ]
        }
      ],
      "source": [
        "processor, log_file, task_suite, num_tasks_in_suite, resize_size = setup_logging_and_environment(model_cfg, policy_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Warning]: datasets path /mnt/sda/home/zijianwang/LIBERO/libero/libero/../datasets does not exist!\n"
          ]
        }
      ],
      "source": [
        "task = task_suite.get_task(model_cfg.task_num)\n",
        "env, task_description = get_libero_env(task, model_cfg.model_family, resolution=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "put both the cream cheese box and the butter in the basket\n"
          ]
        }
      ],
      "source": [
        "print(task_description)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[*] Setting up dataset and data loader...\n",
            "Found 100 success trajectories\n",
            "Task distribution: [('7', 10), ('2', 10), ('1', 10), ('6', 10), ('5', 10), ('3', 10), ('9', 10), ('8', 10), ('0', 10), ('4', 10)]\n",
            "Dataset created with 10 trajectory pairs\n"
          ]
        }
      ],
      "source": [
        "def setup_data_loader(cfg, processor, model, env, task_suite, resize_size, human_prompt_template = \"What action should the robot take to {lang}?\"):\n",
        "    \"\"\"Setup the training data loader.\"\"\"\n",
        "    print(\"[*] Setting up dataset and data loader...\")\n",
        "    \n",
        "    # Create dataset instance\n",
        "    dataset = TrajectoryDataset(\n",
        "        cfg, \n",
        "        cfg.winner_trajectory_path, \n",
        "        cfg.task_suite_name, \n",
        "        processor, \n",
        "        env, \n",
        "        task_suite,\n",
        "        device=cfg.device, \n",
        "        model=model, \n",
        "        img_size=resize_size,\n",
        "        stream_length=cfg.stream_length,\n",
        "        task_num=cfg.task_num,\n",
        "        if_fixed_stream_length = True,\n",
        "        human_prompt_template=human_prompt_template\n",
        "    )\n",
        "    \n",
        "    # Create data collator\n",
        "    data_collator = DataCollatorForPreference(pad_token_id=processor.tokenizer.pad_token_id)\n",
        "    \n",
        "    # Create data loader\n",
        "    train_dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=cfg.batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=data_collator\n",
        "    )\n",
        "    \n",
        "    print(f\"Dataset created with {len(dataset)} trajectory pairs\")\n",
        "    return train_dataloader\n",
        "\n",
        "# human_prompt_template = \"What action should the robot take to {lang}?\"\n",
        "human_prompt_template = \"What sequence of actions should the robot take to {lang}?\"\n",
        "\n",
        "train_dataloader = setup_data_loader(model_cfg, processor, policy_model, env, task_suite, resize_size, human_prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[*] Verifying data loader setup...\n",
            "[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "The if_fixed_stream_length is True, the stream_length is **5**\n",
            "[Warning]: datasets path /mnt/sda/home/zijianwang/LIBERO/libero/libero/../datasets does not exist!\n",
            "**Prompt**: In: What sequence of actions should the robot take to put both the cream cheese box and the butter in the basket??\n",
            "Out:\n",
            "Batch keys: dict_keys(['prompt_input_ids', 'prompt_attention_mask', 'chosen_input_ids', 'chosen_attention_mask', 'rejected_input_ids', 'rejected_attention_mask', 'pixel_values', 'distance', 'start_idx'])\n",
            "Chosen input shape: torch.Size([1, 40])\n",
            "Pixel values shape: torch.Size([1, 6, 224, 224])\n"
          ]
        }
      ],
      "source": [
        "print(\"[*] Verifying data loader setup...\")\n",
        "test_batch = next(iter(train_dataloader))\n",
        "print(f\"Batch keys: {test_batch.keys()}\")\n",
        "print(f\"Chosen input shape: {test_batch['chosen_input_ids'].shape}\")\n",
        "print(f\"Pixel values shape: {test_batch['pixel_values'].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m15652388600\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "/home/zijianwang/miniconda3/envs/openvla/lib/python3.10/site-packages/wandb/analytics/sentry.py:258: DeprecationWarning: The `Scope.user` setter is deprecated in favor of `Scope.set_user()`.\n",
            "  self.scope.user = {\"email\": email}\n",
            "/home/zijianwang/miniconda3/envs/openvla/lib/python3.10/site-packages/wandb/analytics/sentry.py:258: DeprecationWarning: The `Scope.user` setter is deprecated in favor of `Scope.set_user()`.\n",
            "  self.scope.user = {\"email\": email}\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/mnt/sda/home/zijianwang/openvla/vla-scripts/DPO/wandb/run-20250828_173545-e9voub8y</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/15652388600/openvla_CoA_DPO_demo/runs/e9voub8y' target=\"_blank\">DPO+openvla-7b+libero_10_no_noops+task1+b1+lr-0.0001+lora-r48+dropout-0.0--2025-08-28_17-35-44--notebook_demo</a></strong> to <a href='https://wandb.ai/15652388600/openvla_CoA_DPO_demo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/15652388600/openvla_CoA_DPO_demo' target=\"_blank\">https://wandb.ai/15652388600/openvla_CoA_DPO_demo</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/15652388600/openvla_CoA_DPO_demo/runs/e9voub8y' target=\"_blank\">https://wandb.ai/15652388600/openvla_CoA_DPO_demo/runs/e9voub8y</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Policy model device: cuda:0\n",
            "Reference model device: cuda:1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "************Begin to train************\n",
            "[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "The if_fixed_stream_length is True, the stream_length is **20**\n",
            "[Warning]: datasets path /mnt/sda/home/zijianwang/LIBERO/libero/libero/../datasets does not exist!\n",
            "-----This is the 0th batch----\n",
            "Current action stream length: 20\n",
            "     Policy chosen prediction differs from true at token position: 0\n",
            "Group_accuracy: tensor([0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1.], device='cuda:0')\n",
            "Batch 0: First incorrect group at position 0\n",
            "Batch 0: Using DPO loss from group 0: 0.6914 + SFT loss: 11.4396\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 1/100 [00:25<41:53, 25.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved adapter to /mnt/sda/home/zijianwang/openvla/DPO_adapter_tmp_dir/openvla-7b+libero_10_no_noops+task1+b1+lr-0.0001+lora-r48+dropout-0.0--2025-08-28_17-35-44--notebook_demo/ckpt-0, batch_idx: 0\n",
            "[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "The if_fixed_stream_length is True, the stream_length is **20**\n",
            "[Warning]: datasets path /mnt/sda/home/zijianwang/LIBERO/libero/libero/../datasets does not exist!\n",
            "-----This is the 1th batch----\n",
            "Current action stream length: 20\n",
            "     Policy chosen prediction differs from true at token position: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 2/100 [00:56<46:37, 28.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Group_accuracy: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1.], device='cuda:0')\n",
            "Batch 1: All groups correct (accuracy=1), using only SFT loss: 12.3955\n",
            "[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "The if_fixed_stream_length is True, the stream_length is **20**\n",
            "[Warning]: datasets path /mnt/sda/home/zijianwang/LIBERO/libero/libero/../datasets does not exist!\n",
            "-----This is the 2th batch----\n",
            "Current action stream length: 20\n",
            "     Policy chosen prediction differs from true at token position: 0\n",
            "Group_accuracy: tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1.], device='cuda:0')\n",
            "Batch 2: First incorrect group at position 0\n",
            "Batch 2: Using DPO loss from group 0: 6.0000 + SFT loss: 12.5192\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 3/100 [01:20<43:06, 26.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "The if_fixed_stream_length is True, the stream_length is **20**\n",
            "[Warning]: datasets path /mnt/sda/home/zijianwang/LIBERO/libero/libero/../datasets does not exist!\n",
            "-----This is the 3th batch----\n",
            "Current action stream length: 20\n",
            "     Policy chosen prediction differs from true at token position: 0\n",
            "Group_accuracy: tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1.], device='cuda:0')\n",
            "Batch 3: First incorrect group at position 0\n",
            "Batch 3: Using DPO loss from group 0: 4.5000 + SFT loss: 7.8637\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▍         | 4/100 [01:48<43:31, 27.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "The if_fixed_stream_length is True, the stream_length is **20**\n",
            "[Warning]: datasets path /mnt/sda/home/zijianwang/LIBERO/libero/libero/../datasets does not exist!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                               \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[*] Training interrupted by user\n",
            "Error in callback <bound method _WandbInit._post_run_cell_hook of <wandb.sdk.wandb_init._WandbInit object at 0x7fc5082ab550>> (for post_run_cell), with arguments args (<ExecutionResult object at 7fc5082aa8c0, execution_count=8 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7fc5082a9120, raw_cell=\"model_cfg.use_wandb = True\n",
            "print(model_cfg.use_wan..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell://ssh-remote%2Bxuchang-lab0/mnt/sda/home/zijianwang/openvla/vla-scripts/DPO/dpo_demo_pro.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D> result=None>,),kwargs {}:\n"
          ]
        },
        {
          "ename": "BrokenPipeError",
          "evalue": "[Errno 32] Broken pipe",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
            "File \u001b[0;32m~/miniconda3/envs/openvla/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:593\u001b[0m, in \u001b[0;36m_WandbInit._post_run_cell_hook\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresuming backend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 593\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/openvla/lib/python3.10/site-packages/wandb/sdk/interface/interface.py:788\u001b[0m, in \u001b[0;36mInterfaceBase.publish_resume\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpublish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    787\u001b[0m     resume \u001b[38;5;241m=\u001b[39m pb\u001b[38;5;241m.\u001b[39mResumeRequest()\n\u001b[0;32m--> 788\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/openvla/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py:293\u001b[0m, in \u001b[0;36mInterfaceShared._publish_resume\u001b[0;34m(self, resume)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_publish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m, resume: pb\u001b[38;5;241m.\u001b[39mResumeRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    292\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(resume\u001b[38;5;241m=\u001b[39mresume)\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/openvla/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py:39\u001b[0m, in \u001b[0;36mInterfaceSock._publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb.Record\u001b[39m\u001b[38;5;124m\"\u001b[39m, local: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign(record)\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_record_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/openvla/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:174\u001b[0m, in \u001b[0;36mSockClient.send_record_publish\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    172\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrequest_id \u001b[38;5;241m=\u001b[39m record\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mmailbox_slot\n\u001b[1;32m    173\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrecord_publish\u001b[38;5;241m.\u001b[39mCopyFrom(record)\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_server_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_req\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/openvla/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:154\u001b[0m, in \u001b[0;36mSockClient.send_server_request\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend_server_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg: spb\u001b[38;5;241m.\u001b[39mServerRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/openvla/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:151\u001b[0m, in \u001b[0;36mSockClient._send_message\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    149\u001b[0m header \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<BI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m), raw_size)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendall_with_error_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/openvla/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:130\u001b[0m, in \u001b[0;36mSockClient._sendall_with_error_handle\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    128\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     sent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# sent equal to 0 indicates a closed socket\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sent \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
          ]
        }
      ],
      "source": [
        "model_cfg.use_wandb = True\n",
        "print(model_cfg.use_wandb)\n",
        "try:\n",
        "    final_adapter_dir = train_dpo(\n",
        "        model=policy_model, \n",
        "        ref_model=ref_model, \n",
        "        train_dataloader=train_dataloader, \n",
        "        cfg=model_cfg, \n",
        "        if_not_demo=model_cfg.use_wandb\n",
        "    )\n",
        "    \n",
        "    print(f\"[*] Training completed successfully!\")\n",
        "    print(f\"[*] Final adapter saved to: {final_adapter_dir}\")\n",
        "    \n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n[*] Training interrupted by user\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"[*] Training failed with error: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "group_accuracy = torch.tensor([1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.])\n",
        "torch.where(group_accuracy == 0.0)\n",
        "\n",
        "\n",
        "a = np.allclose([0.056, 0.028, 0.446], [0.056, 0.028, 0.446], atol=1e-6)\n",
        "print(a)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "openvla",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
