{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a55d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb6369d",
   "metadata": {},
   "source": [
    "# 1. 修改model的load, 是base+lora的形式, 仅train lora 1\n",
    "# 2. dataset不仅返回prompt_ids, 还有pixe values. 2\n",
    "# 3. forward算logit时, action id, 也得转为embeddings. 3\n",
    "# 4. 确定轨迹的stream length 4 \n",
    "# 5. 先拿一个batch的winner, 再batch前向传播, 得到loser轨迹 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d99cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import DPOConfig, DPOTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import AutoConfig, AutoImageProcessor, AutoModelForVision2Seq, AutoProcessor\n",
    "from peft import PeftModel, PeftConfig\n",
    "from src.data_process import TrajectoryDataset\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union\n",
    "import draccus\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from experiments.robot.robot_utils import get_model\n",
    "from libero.libero import benchmark\n",
    "import wandb\n",
    "from experiments.robot.libero.libero_utils import (\n",
    "    get_libero_dummy_action,\n",
    "    get_libero_env,\n",
    "    get_libero_image,\n",
    "    quat2axisangle,\n",
    "    save_rollout_video_CoA,\n",
    ")\n",
    "from experiments.robot.openvla_utils import get_processor, get_input\n",
    "from experiments.robot.robot_utils import (\n",
    "    DATE_TIME,\n",
    "    get_action,\n",
    "    get_CoA,\n",
    "    get_image_resize_size,\n",
    "    get_model,\n",
    "    get_vla_via_lora,\n",
    "    invert_gripper_action,\n",
    "    normalize_gripper_action,\n",
    "    set_seed_everywhere,\n",
    ")\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union\n",
    "import torch\n",
    "from torch.utils.data import Dataset, IterableDataset\n",
    "from transformers import PreTrainedTokenizerBase, BaseImageProcessor, FeatureExtractionMixin, ProcessorMixin\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0723ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GenerateConfig:\n",
    "\n",
    "    # fmt: off\n",
    "    vla_path: str = \"openvla/openvla-7b\" \n",
    "    # root_dir: str = \"/hdd/zijianwang\"\n",
    "    root_dir: str = \"/mnt/sda/home/zijianwang\"\n",
    "\n",
    "\n",
    "    #################################################################################################################\n",
    "    # LoRA parameters\n",
    "    #################################################################################################################\n",
    "    use_lora: bool = True\n",
    "    lora_rank: int = 48\n",
    "    lora_dropout: float = 0.0\n",
    "    \n",
    "    #################################################################################################################\n",
    "    # Model-specific parameters\n",
    "    #################################################################################################################\n",
    "    model_family: str = \"openvla\"                    # Model family\n",
    "\n",
    "    dataset_name: str = \"libero_10_no_noops\"\n",
    "\n",
    "    pretrained_checkpoint: Union[str, Path] = os.path.join(root_dir, \"openvla/FT_res/openvla-7b-finetuned-libero-10+libero_10_no_noops+b4+lr-0.0005+lora-r48+dropout-0.0--image_aug--2025-07-18_19-26-25\")     # Pretrained checkpoint path\n",
    "    lora_path: str = os.path.join(root_dir, \"openvla/adapter_tmp_dir/openvla-7b-finetuned-libero-10+libero_10_no_noops+b4+lr-0.0005+lora-r48+dropout-0.0--image_aug--2025-07-18_19-26-25\")\n",
    "    base_vla_path: str = os.path.join(root_dir, \"HF_CACHE/openvla-7b-finetuned-libero-10\")\n",
    "\n",
    "    winner_trajectory_path: str = os.path.join(root_dir, \"openvla/vla-scripts/DPO/winner_trajectory\")\n",
    "\n",
    "    adapter_tmp_dir: str = os.path.join(root_dir, \"openvla/adapter_tmp_dir\")\n",
    "    run_root_dir: str = os.path.join(root_dir, \"openvla/DPO_res\")\n",
    "\n",
    "    #################################################################################################################\n",
    "    load_in_8bit: bool = False                       # (For OpenVLA only) Load with 8-bit quantization\n",
    "    load_in_4bit: bool = False                       # (For OpenVLA only) Load with 4-bit quantization\n",
    "    center_crop: bool = True                         # Center crop? (if trained w/ random crop image aug)\n",
    "    #################################################################################################################\n",
    "    # Training parameters\n",
    "    #################################################################################################################\n",
    "    batch_size: int = 4\n",
    "    grad_accumulation_steps: int = 1\n",
    "    learning_rate: float = 0.0005\n",
    "    max_steps: int = 10000\n",
    "    dpo_beta: float = 0.1\n",
    "    #################################################################################################################\n",
    "    # LIBERO environment-specific parameters\n",
    "    #################################################################################################################\n",
    "    task_suite_name: str = \"libero_10\"          # Task suite. Options: libero_spatial, libero_object, libero_goal, libero_10, libero_90\n",
    "    num_steps_wait: int = 10                         # Number of steps to wait for objects to stabilize in sim\n",
    "    num_trials_per_task: int = 50                    # Number of rollouts per task\n",
    "    unnorm_key = task_suite_name\n",
    "    #################################################################################################################\n",
    "    # Utils\n",
    "    #################################################################################################################\n",
    "    run_id_note: Optional[str] = None                # Extra note to add in run ID for logging\n",
    "    local_log_dir: str = \"./experiments/logs\"        # Local directory for eval logs\n",
    "\n",
    "    use_wandb: bool = False                          # Whether to also log results in Weights & Biases\n",
    "    wandb_project: str = \"YOUR_WANDB_PROJECT\"        # Name of W&B project to log to (use default!)\n",
    "    wandb_entity: str = \"YOUR_WANDB_ENTITY\"          # Name of entity to log under\n",
    "\n",
    "    seed: int = 7                                    # Random Seed (for reproducibility)\n",
    "\n",
    "    device: str = \"cuda:2\"\n",
    "\n",
    "\n",
    "    wandb_project = \"openvla_CoA_DPO\"\n",
    "    wandb_entity = \"15652388600\"\n",
    "\n",
    "    # fmt: on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766f3fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model_and_config(cfg: GenerateConfig):\n",
    "    \"\"\"Setup and validate configuration, then load the model.\"\"\"\n",
    "    assert cfg.pretrained_checkpoint is not None, \"cfg.pretrained_checkpoint must not be None!\"\n",
    "    if \"image_aug\" in cfg.pretrained_checkpoint:\n",
    "        assert cfg.center_crop, \"Expecting `center_crop==True` because model was trained with image augmentations!\"\n",
    "    assert not (cfg.load_in_8bit and cfg.load_in_4bit), \"Cannot use both 8-bit and 4-bit quantization!\"\n",
    "\n",
    "    # Set random seed\n",
    "    set_seed_everywhere(cfg.seed)\n",
    "\n",
    "    cfg.unnorm_key = cfg.task_suite_name\n",
    "\n",
    "    # Load model\n",
    "    model = get_model(cfg)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def setup_logging_and_environment(cfg: GenerateConfig, model):\n",
    "    \"\"\"Setup logging and LIBERO environment.\"\"\"\n",
    "    # [OpenVLA] Check that the model contains the action un-normalization key\n",
    "    if cfg.model_family == \"openvla\":\n",
    "        # In some cases, the key must be manually modified (e.g. after training on a modified version of the dataset\n",
    "        # with the suffix \"_no_noops\" in the dataset name)\n",
    "        if cfg.unnorm_key not in model.norm_stats and f\"{cfg.unnorm_key}_no_noops\" in model.norm_stats:\n",
    "            cfg.unnorm_key = f\"{cfg.unnorm_key}_no_noops\"\n",
    "        assert cfg.unnorm_key in model.norm_stats, f\"Action un-norm key {cfg.unnorm_key} not found in VLA `norm_stats`!\"\n",
    "\n",
    "    # [OpenVLA] Get Hugging Face processor\n",
    "    processor = None\n",
    "    if cfg.model_family == \"openvla\":\n",
    "        processor = get_processor(cfg)\n",
    "\n",
    "    # Initialize local logging\n",
    "    run_id = f\"EVAL-{cfg.task_suite_name}-{cfg.model_family}-{DATE_TIME}\"\n",
    "    if cfg.run_id_note is not None:\n",
    "        run_id += f\"--{cfg.run_id_note}\"\n",
    "    os.makedirs(cfg.local_log_dir, exist_ok=True)\n",
    "    local_log_filepath = os.path.join(cfg.local_log_dir, run_id + \".txt\")\n",
    "    log_file = open(local_log_filepath, \"w\")\n",
    "    print(f\"Logging to local log file: {local_log_filepath}\")\n",
    "\n",
    "    # Initialize Weights & Biases logging as well\n",
    "    if cfg.use_wandb:\n",
    "        wandb.init(\n",
    "            entity=cfg.wandb_entity,\n",
    "            project=cfg.wandb_project,\n",
    "            name=run_id,\n",
    "        )\n",
    "\n",
    "    # Initialize LIBERO task suite\n",
    "    benchmark_dict = benchmark.get_benchmark_dict()\n",
    "    task_suite = benchmark_dict[cfg.task_suite_name]()\n",
    "    num_tasks_in_suite = task_suite.n_tasks\n",
    "    print(f\"Task suite: {cfg.task_suite_name}\")\n",
    "    log_file.write(f\"Task suite: {cfg.task_suite_name}\\n\")\n",
    "\n",
    "    # Get expected image dimensions\n",
    "    resize_size = get_image_resize_size(cfg)\n",
    "\n",
    "    return processor, log_file, task_suite, num_tasks_in_suite, resize_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8029504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Main function to run the OpenVLA LIBERO inference demo.\"\"\"\n",
    "print(\"[*] Starting OpenVLA LIBERO Inference Demo\")\n",
    "\n",
    "# Initialize configuration\n",
    "model_cfg = GenerateConfig(device = \"cuda:3\")\n",
    "\n",
    "# Setup model and configuration\n",
    "print(\"[*] Loading model and setting up configuration...\")\n",
    "# model = setup_model_and_config(cfg)\n",
    "\n",
    "model = get_vla_via_lora(model_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb3decc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging and environment\n",
    "print(\"[*] Setting up logging and environment...\")\n",
    "processor, log_file, task_suite, num_tasks_in_suite, resize_size = setup_logging_and_environment(model_cfg, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c3096e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_config = GenerateConfig(device = \"cuda:0\")\n",
    "print(ref_config.device)\n",
    "ref_model = get_model(ref_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710d916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset instance\n",
    "dataset = TrajectoryDataset(model_cfg, model_cfg.winner_trajectory_path, model_cfg.task_suite_name, processor, device = model_cfg.device, model = model, img_size = resize_size, stream_length = 3)\n",
    "\n",
    "# dataset只返回\"prompt_input_ids\": [1, 2, 3], \"chosen_input_ids\": [4, 5], \"rejected_input_ids\": [6], \n",
    "# attention_mask在DataCollatorForPreference生成, labels在dpotrainer.concatenated_forward中生成. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f551896b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl.trainer.dpo_trainer import DataCollatorForPreference\n",
    "from torch.utils.data import DataLoader\n",
    "model_cfg = GenerateConfig()\n",
    "data_collator = DataCollatorForPreference(pad_token_id = processor.tokenizer.pad_token_id)\n",
    "train_dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=model_cfg.batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator  # 需要返回上述格式的batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b07991",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch['chosen_input_ids'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd925be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch.keys())\n",
    "print(batch['chosen_input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69657e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    pred_test = model.forward(input_ids = batch[\"prompt_input_ids\"], attention_mask = batch[\"chosen_attention_mask\"], pixel_values=batch[\"pixel_values\"])\n",
    "\n",
    "print(pred_test.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c7bf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_log_probs(model, prompt_input_ids, completion_input_ids, prompt_attention_mask, completion_attention_mask, model_device=None):\n",
    "    \"\"\"计算模型对completion部分的log概率\"\"\"\n",
    "    # 确保输入数据在正确的device上\n",
    "    if model_device is None:\n",
    "        model_device = next(model.parameters()).device\n",
    "    \n",
    "    prompt_input_ids = prompt_input_ids.to(model_device)\n",
    "    completion_input_ids = completion_input_ids.to(model_device)\n",
    "    prompt_attention_mask = prompt_attention_mask.to(model_device)\n",
    "    completion_attention_mask = completion_attention_mask.to(model_device)\n",
    "    \n",
    "    # 拼接prompt和completion\n",
    "    input_ids = torch.cat([prompt_input_ids, completion_input_ids], dim=1)\n",
    "    attention_mask = torch.cat([prompt_attention_mask, completion_attention_mask], dim=1)\n",
    "    \n",
    "    # 前向传播\n",
    "    with torch.no_grad() if model.training == False else torch.enable_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, use_cache=False)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # 计算completion部分的log概率\n",
    "    prompt_len = prompt_input_ids.shape[1]\n",
    "    completion_logits = logits[:, prompt_len-1:-1, :]  # 获取completion部分的logits\n",
    "    completion_labels = completion_input_ids\n",
    "    \n",
    "    # 计算每个token的log概率\n",
    "    log_probs = F.log_softmax(completion_logits, dim=-1)\n",
    "    # 获取对应label的log概率\n",
    "    selected_log_probs = torch.gather(log_probs, dim=-1, index=completion_labels.unsqueeze(-1)).squeeze(-1)\n",
    "    # 对非padding的token求和\n",
    "    sequence_log_probs = (selected_log_probs * completion_attention_mask).sum(dim=-1)\n",
    "    \n",
    "    return sequence_log_probs\n",
    "\n",
    "def dpo_loss(policy_chosen_logps, policy_rejected_logps, \n",
    "             ref_chosen_logps, ref_rejected_logps, beta=0.1, target_device=None):\n",
    "    \"\"\"计算DPO损失，处理不同device上的tensor\"\"\"\n",
    "    \n",
    "    # 确定目标device (通常是policy model的device)\n",
    "    if target_device is None:\n",
    "        target_device = policy_chosen_logps.device\n",
    "    \n",
    "    # 将所有tensor移动到目标device\n",
    "    policy_chosen_logps = policy_chosen_logps.to(target_device)\n",
    "    policy_rejected_logps = policy_rejected_logps.to(target_device)\n",
    "    ref_chosen_logps = ref_chosen_logps.to(target_device)\n",
    "    ref_rejected_logps = ref_rejected_logps.to(target_device)\n",
    "    \n",
    "    # 计算log比率\n",
    "    chosen_logratios = policy_chosen_logps - ref_chosen_logps\n",
    "    rejected_logratios = policy_rejected_logps - ref_rejected_logps\n",
    "    \n",
    "    # DPO损失: -log(sigmoid(beta * (chosen_logratios - rejected_logratios)))\n",
    "    logits = chosen_logratios - rejected_logratios\n",
    "    losses = -F.logsigmoid(beta * logits)\n",
    "    \n",
    "    # 计算奖励（用于监控）\n",
    "    chosen_rewards = beta * chosen_logratios.detach()\n",
    "    rejected_rewards = beta * rejected_logratios.detach()\n",
    "    \n",
    "    return losses.mean(), chosen_rewards, rejected_rewards\n",
    "\n",
    "def move_batch_to_device(batch, device):\n",
    "    \"\"\"将batch中的所有tensor移动到指定device\"\"\"\n",
    "    moved_batch = {}\n",
    "    for key, value in batch.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            moved_batch[key] = value.to(device)\n",
    "        else:\n",
    "            moved_batch[key] = value\n",
    "    return moved_batch\n",
    "\n",
    "# 训练循环\n",
    "def train_dpo(model, ref_model, train_dataloader, cfg, if_not_demo = False):\n",
    "    \"\"\"DPO训练主循环，支持不同device上的模型\"\"\"\n",
    "\n",
    "    # Configure Unique Experiment ID & Log Directory\n",
    "    exp_id = (\n",
    "        f\"{cfg.vla_path.split('/')[-1]}+{cfg.dataset_name}\"\n",
    "        f\"+b{cfg.batch_size * cfg.grad_accumulation_steps}\"\n",
    "        f\"+lr-{cfg.learning_rate}\"\n",
    "    )\n",
    "    if cfg.use_lora:\n",
    "        exp_id += f\"+lora-r{cfg.lora_rank}+dropout-{cfg.lora_dropout}\"\n",
    "    import datetime\n",
    "    exp_id += f\"--{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "    if cfg.run_id_note is not None:\n",
    "        exp_id += f\"--{cfg.run_id_note}\"\n",
    "\n",
    "    run_dir, adapter_dir = os.path.join(cfg.run_root_dir, exp_id), os.path.join(cfg.adapter_tmp_dir, exp_id)\n",
    "    if if_not_demo:\n",
    "        wandb.init(entity=cfg.wandb_entity, project=cfg.wandb_project, name=f\"dpo+{exp_id}\")\n",
    "    \n",
    "    # 获取模型所在的device\n",
    "    policy_device = next(model.parameters()).device\n",
    "    ref_device = next(ref_model.parameters()).device\n",
    "    \n",
    "    print(f\"Policy model device: {policy_device}\")\n",
    "    print(f\"Reference model device: {ref_device}\")\n",
    "    \n",
    "    # 设置优化器\n",
    "    optimizer = AdamW(model.parameters(), lr=cfg.learning_rate)\n",
    "    \n",
    "    # 确保参考模型不更新\n",
    "    ref_model.eval()\n",
    "    for param in ref_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    from collections import deque\n",
    "    # Deque to store recent train metrics (used for computing smoothened metrics for gradient accumulation)\n",
    "    recent_losses = deque(maxlen=cfg.grad_accumulation_steps)\n",
    "    recent_accuracies = deque(maxlen=cfg.grad_accumulation_steps)\n",
    "    recent_rewards_margin = deque(maxlen=cfg.grad_accumulation_steps)\n",
    "    recent_chosen_logps = deque(maxlen=cfg.grad_accumulation_steps)\n",
    "    recent_rejected_logps = deque(maxlen=cfg.grad_accumulation_steps)\n",
    "    recent_chosen_rewards = deque(maxlen=cfg.grad_accumulation_steps)\n",
    "    recent_rejected_rewards = deque(maxlen=cfg.grad_accumulation_steps)\n",
    "    \n",
    "    # 训练循环\n",
    "    with tqdm(total = cfg.max_steps, leave=False) as progress:\n",
    "        model.train()\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_dataloader):\n",
    "            # 将batch移动到policy model的device\n",
    "            policy_batch = move_batch_to_device(batch, policy_device)\n",
    "            \n",
    "            # 1. 计算策略模型的log概率 (在policy_device上)\n",
    "            policy_chosen_logps = compute_log_probs(\n",
    "                model, \n",
    "                policy_batch['prompt_input_ids'], \n",
    "                policy_batch['chosen_input_ids'], \n",
    "                policy_batch['prompt_attention_mask'], \n",
    "                policy_batch['chosen_attention_mask'],\n",
    "                model_device = policy_device\n",
    "            )\n",
    "            policy_rejected_logps = compute_log_probs(\n",
    "                model, \n",
    "                policy_batch['prompt_input_ids'], \n",
    "                policy_batch['rejected_input_ids'],\n",
    "                policy_batch['prompt_attention_mask'], \n",
    "                policy_batch['rejected_attention_mask'],\n",
    "                model_device = policy_device\n",
    "            )\n",
    "            \n",
    "            # 2. 计算参考模型的log概率 (在ref_device上)\n",
    "            with torch.no_grad():\n",
    "                # 将batch移动到ref model的device\n",
    "                ref_batch = move_batch_to_device(batch, ref_device)\n",
    "                \n",
    "                ref_chosen_logps = compute_log_probs(\n",
    "                    ref_model, \n",
    "                    ref_batch['prompt_input_ids'], \n",
    "                    ref_batch['chosen_input_ids'],\n",
    "                    ref_batch['prompt_attention_mask'], \n",
    "                    ref_batch['chosen_attention_mask'],\n",
    "                    model_device = ref_device\n",
    "                )\n",
    "                ref_rejected_logps = compute_log_probs(\n",
    "                    ref_model, \n",
    "                    ref_batch['prompt_input_ids'], \n",
    "                    ref_batch['rejected_input_ids'],\n",
    "                    ref_batch['prompt_attention_mask'], \n",
    "                    ref_batch['rejected_attention_mask'],\n",
    "                    model_device = ref_device\n",
    "                )\n",
    "            \n",
    "            # 3. 计算DPO损失 (在policy_device上)\n",
    "            loss, chosen_rewards, rejected_rewards = dpo_loss(\n",
    "                policy_chosen_logps, policy_rejected_logps,\n",
    "                ref_chosen_logps, ref_rejected_logps,\n",
    "                beta = cfg.dpo_beta, target_device = policy_device\n",
    "            )\n",
    "            \n",
    "            # 4. 反向传播和优化\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # 梯度裁剪（可选）\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = 1.0)\n",
    "            optimizer.step()\n",
    "            progress.update()\n",
    "\n",
    "            # 5. 计算指标\n",
    "            accuracy = (chosen_rewards > rejected_rewards).float().mean()\n",
    "            reward_margin = (chosen_rewards - rejected_rewards).mean()\n",
    "\n",
    "            recent_losses.append(loss.item())\n",
    "            recent_accuracies.append(accuracy.item())\n",
    "            recent_rewards_margin.append(reward_margin.item())\n",
    "            recent_chosen_logps.append(policy_chosen_logps.mean().item())\n",
    "            recent_rejected_logps.append(policy_rejected_logps.mean().item())\n",
    "            recent_chosen_rewards.append(chosen_rewards.mean().item())\n",
    "            recent_rejected_rewards.append(rejected_rewards.mean().item())\n",
    "\n",
    "            # Compute smoothened train metrics\n",
    "            #   =>> Equal to current step metrics when not using gradient accumulation\n",
    "            #   =>> Otherwise, equal to the average of metrics observed over micro-batches used for gradient accumulation\n",
    "            smoothened_loss = sum(recent_losses) / len(recent_losses)\n",
    "            smoothened_accuracy = sum(recent_accuracies) / len(recent_accuracies)\n",
    "            smoothened_rewards_margin = sum(recent_rewards_margin) / len(recent_rewards_margin)\n",
    "            smoothened_chosen_logps = sum(recent_chosen_logps) / len(recent_chosen_logps)\n",
    "            smoothened_rejected_logps = sum(recent_rejected_logps) / len(recent_rejected_logps)\n",
    "            smoothened_chosen_rewards = sum(recent_chosen_rewards) / len(recent_chosen_rewards)\n",
    "            smoothened_rejected_rewards = sum(recent_rejected_rewards) / len(recent_rejected_rewards)\n",
    "\n",
    "            # Compute gradient step index\n",
    "            gradient_step_idx = batch_idx // cfg.grad_accumulation_steps\n",
    "\n",
    "            if gradient_step_idx % 1 == 0:\n",
    "                if if_not_demo:\n",
    "                    wandb.log({\n",
    "                        \"loss\": smoothened_loss,\n",
    "                        \"accuracy\": smoothened_accuracy,\n",
    "                        \"reward_margin\": smoothened_rewards_margin,\n",
    "                        \"chosen_logps\": smoothened_chosen_logps,\n",
    "                        \"rejected_logps\": smoothened_rejected_logps,\n",
    "                        \"chosen_rewards\": smoothened_chosen_rewards,\n",
    "                        \"rejected_rewards\": smoothened_rejected_rewards\n",
    "                    },\n",
    "                    step=gradient_step_idx,\n",
    "                    )\n",
    "            \n",
    "            if gradient_step_idx % 10 == 0:\n",
    "                model.save_pretrained(adapter_dir)\n",
    "                print(f\"Saved adapter to {adapter_dir}, batch_idx: {batch_idx}\")\n",
    "                \n",
    "            # 定期清理缓存（如果使用GPU）\n",
    "            if gradient_step_idx % 10 == 0:\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5affa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用示例，处理不同device的情况\n",
    "if __name__ == \"__main__\":\n",
    "    # 示例：将ref_model放在CPU上以节省GPU内存\n",
    "    \n",
    "    # 或者在多GPU环境下，将模型放在不同GPU上\n",
    "    # model = model.to('cuda:0')      # 策略模型在GPU 0\n",
    "    # ref_model = ref_model.to('cuda:1')  # 参考模型在GPU 1\n",
    "\n",
    "    model_cfg = GenerateConfig(max_steps = 10000, grad_accumulation_steps = 1, dpo_beta = 0.1)\n",
    "    # 开始训练\n",
    "    train_dpo(model, ref_model, train_dataloader, model_cfg, if_not_demo = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ac10a3",
   "metadata": {},
   "source": [
    "# ---------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
