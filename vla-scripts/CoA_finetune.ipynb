{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zijianwang/miniconda3/envs/openvla/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-07-25 21:48:52.189942: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-25 21:48:52.189981: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-25 21:48:52.191787: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-25 21:48:52.200698: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-25 21:48:53.148244: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"/mnt/sda/home/zijianwang/HF_CACHE\"\n",
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import draccus\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import tqdm\n",
    "from accelerate import PartialState\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig\n",
    "from transformers import AutoConfig, AutoImageProcessor\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "import wandb\n",
    "from prismatic.models.backbones.llm.prompting import PurePromptBuilder, VicunaV15ChatPromptBuilder\n",
    "from prismatic.util.data_utils import PaddedCollatorForActionPrediction\n",
    "from prismatic.vla.action_tokenizer import ActionTokenizer\n",
    "from prismatic.vla.datasets import RLDSBatchTransform, RLDSDataset, EpisodicRLDSDataset\n",
    "from prismatic.vla.datasets.rlds.utils.data_utils import save_dataset_statistics\n",
    "\n",
    "from prismatic.extern.hf.configuration_prismatic import OpenVLAConfig\n",
    "from prismatic.extern.hf.modeling_prismatic import OpenVLAForActionPrediction\n",
    "from prismatic.extern.hf.processing_prismatic import PrismaticImageProcessor, PrismaticProcessor\n",
    "\n",
    "# Sane Defaults\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AutoConfig.register(\"openvla\", OpenVLAConfig)\n",
    "AutoImageProcessor.register(OpenVLAConfig, PrismaticImageProcessor)\n",
    "AutoProcessor.register(OpenVLAConfig, PrismaticProcessor)\n",
    "AutoModelForVision2Seq.register(OpenVLAConfig, OpenVLAForActionPrediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vla_model_config = OpenVLAConfig.from_pretrained(\"openvla/openvla-7b\")\n",
    "processor = AutoProcessor.from_pretrained(\"openvla/openvla-7b\", trust_remote_code=True)\n",
    "action_tokenizer = ActionTokenizer(processor.tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add action unit separate token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokenizer): 32001\n",
      "len(tokenizer): 32002\n",
      "id of <A>: 32001\n"
     ]
    }
   ],
   "source": [
    "print(\"len(tokenizer):\", len(processor.tokenizer))\n",
    "DEFAULT_ACT_TOKEN = \"<A>\"\n",
    "num_added_toks = processor.tokenizer.add_tokens(DEFAULT_ACT_TOKEN)\n",
    "print(\"len(tokenizer):\", len(processor.tokenizer))\n",
    "print(\"id of <A>:\", processor.tokenizer.convert_tokens_to_ids(\"<A>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of sequence token: </s>\n",
      "End of sequence token id: 2\n",
      "All special tokens: {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<PAD>'}\n"
     ]
    }
   ],
   "source": [
    "print(\"End of sequence token:\", processor.tokenizer.eos_token)\n",
    "print(\"End of sequence token id:\", processor.tokenizer.eos_token_id)\n",
    "print(\"All special tokens:\", processor.tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage of action tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vla = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"/mnt/sda/home/zijianwang/HF_CACHE/openvla-7b-finetuned-libero-10\", \n",
    "    attn_implementation=\"flash_attention_2\",  # [Optional] Requires `flash_attn`\n",
    "    torch_dtype=torch.bfloat16, \n",
    "    low_cpu_mem_usage=True, \n",
    "    trust_remote_code=True\n",
    ").to(\"cuda:3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vla.norm_stats.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vla.resize_token_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 21:49:40.671903: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">07/25 [21:49:41] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> | &gt;&gt; <span style=\"font-weight: bold\">[</span>*<span style=\"font-weight: bold\">]</span> Loading existing dataset statistics from                       <a href=\"file:///mnt/sda/home/zijianwang/openvla/prismatic/vla/datasets/rlds/utils/data_utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">data_utils.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///mnt/sda/home/zijianwang/openvla/prismatic/vla/datasets/rlds/utils/data_utils.py#208\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">208</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         <span style=\"color: #800080; text-decoration-color: #800080\">/mnt/sda/home/zijianwang/openvla/modified_libero_rlds/libero_10_no_noop</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         <span style=\"color: #800080; text-decoration-color: #800080\">s/1.0.0/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">dataset_statistics_54f8906349df96f4db89976b949c60c4bd4450524dfd</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">30752a192e13324b83db.json.</span>                                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m07/25 [21:49:41]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m | >> \u001b[1m[\u001b[0m*\u001b[1m]\u001b[0m Loading existing dataset statistics from                       \u001b]8;id=166389;file:///mnt/sda/home/zijianwang/openvla/prismatic/vla/datasets/rlds/utils/data_utils.py\u001b\\\u001b[2mdata_utils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=161284;file:///mnt/sda/home/zijianwang/openvla/prismatic/vla/datasets/rlds/utils/data_utils.py#208\u001b\\\u001b[2m208\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                 \u001b[0m         \u001b[35m/mnt/sda/home/zijianwang/openvla/modified_libero_rlds/libero_10_no_noop\u001b[0m \u001b[2m                 \u001b[0m\n",
       "\u001b[2;36m                 \u001b[0m         \u001b[35ms/1.0.0/\u001b[0m\u001b[95mdataset_statistics_54f8906349df96f4db89976b949c60c4bd4450524dfd\u001b[0m \u001b[2m                 \u001b[0m\n",
       "\u001b[2;36m                 \u001b[0m         \u001b[95m30752a192e13324b83db.json.\u001b[0m                                              \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 21:49:41.273350: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n"
     ]
    }
   ],
   "source": [
    "batch_transform = RLDSBatchTransform(\n",
    "    action_tokenizer,\n",
    "    processor.tokenizer,\n",
    "    image_transform=processor.image_processor.apply_transform,\n",
    "    prompt_builder_fn=PurePromptBuilder if \"v01\" not in \"openvla/openvla-7b\" else VicunaV15ChatPromptBuilder,\n",
    ")\n",
    "\n",
    "\n",
    "task_name = \"LIBERO-Long\" #LIBERO-Object, LIBERO-Goal, LIBERO-Long, LIBERO-Spatial\"\n",
    "\n",
    "episodic_vla_dataset = EpisodicRLDSDataset(\n",
    "    \"/mnt/sda/home/zijianwang/openvla/modified_libero_rlds\",\n",
    "    \"libero_10_no_noops\",\n",
    "    batch_transform,\n",
    "    resize_resolution=tuple(vla_model_config.image_sizes),\n",
    "    shuffle_buffer_size=100_000,\n",
    "    image_aug=True,\n",
    "    if_random_start=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1753444190.974029 1767125 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"CropAndResize\" attr { key: \"T\" value { type: DT_FLOAT } } attr { key: \"extrapolation_value\" value { f: 0 } } attr { key: \"method\" value { s: \"bilinear\" } } inputs { dtype: DT_FLOAT shape { dim { size: 1 } dim { size: 224 } dim { size: 224 } dim { size: -7 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2500 num_cores: 80 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: -8 } dim { size: -9 } dim { size: -7 } } }\n",
      "W0000 00:00:1753444191.056144 1767125 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"CropAndResize\" attr { key: \"T\" value { type: DT_FLOAT } } attr { key: \"extrapolation_value\" value { f: 0 } } attr { key: \"method\" value { s: \"bilinear\" } } inputs { dtype: DT_FLOAT shape { dim { size: 1 } dim { size: 224 } dim { size: 224 } dim { size: -6 } } } inputs { dtype: DT_FLOAT shape { dim { size: -3 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -3 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2500 num_cores: 80 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -3 } dim { size: -10 } dim { size: -11 } dim { size: -6 } } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# target_index = random.randint(0, len(episodic_vla_dataset))\n",
    "target_index = 0\n",
    "for idx, episode in enumerate(episodic_vla_dataset):\n",
    "    if idx != target_index:\n",
    "        continue\n",
    "    elif idx == target_index:\n",
    "        length = episode['length']\n",
    "    \n",
    "        print(length)\n",
    "\n",
    "        replay_images = episode['replay_images']\n",
    "        break\n",
    "# print(sum_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['pixel_values', 'input_ids', 'labels', 'dataset_name', 'length', 'replay_images', 'text'])\n"
     ]
    }
   ],
   "source": [
    "print(episode.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 224, 224)\n",
      "268\n",
      "<s> In: What action should the robot take to put both the alphabet soup and the tomato sauce in the basket?\n",
      "Out: \n"
     ]
    }
   ],
   "source": [
    "print(replay_images[1].shape)\n",
    "print(len(replay_images))\n",
    "text = episode['text']\n",
    "print(processor.tokenizer.decode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import imageio\n",
    "import numpy as np\n",
    "sys.path.append(\"../..\")\n",
    "# from experiments.robot.libero.libero_utils import save_rollout_video\n",
    "mp4_path = \"/mnt/sda/home/zijianwang/openvla/vla-scripts/demo_video/1.mp4\"\n",
    "video_writer = imageio.get_writer(mp4_path, fps=30)\n",
    "for img in replay_images:\n",
    "    if img.shape[0] == 3 and len(img.shape) == 3:  # (3, H, W) 格式\n",
    "        img = np.transpose(img, (1, 2, 0))  # 转换为 (H, W, 3)\n",
    "        video_writer.append_data(img)\n",
    "video_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataCollatorForCoASupervisedDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/sda/home/zijianwang/openvla/vla-scripts/CoA_finetune.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bxuchang-lab0/mnt/sda/home/zijianwang/openvla/vla-scripts/CoA_finetune.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m collator \u001b[39m=\u001b[39m DataCollatorForCoASupervisedDataset(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bxuchang-lab0/mnt/sda/home/zijianwang/openvla/vla-scripts/CoA_finetune.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     processor\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mmodel_max_length, processor\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mpad_token_id, padding_side\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mright\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bxuchang-lab0/mnt/sda/home/zijianwang/openvla/vla-scripts/CoA_finetune.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bxuchang-lab0/mnt/sda/home/zijianwang/openvla/vla-scripts/CoA_finetune.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m dataloader \u001b[39m=\u001b[39m DataLoader(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bxuchang-lab0/mnt/sda/home/zijianwang/openvla/vla-scripts/CoA_finetune.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     episodic_vla_dataset,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bxuchang-lab0/mnt/sda/home/zijianwang/openvla/vla-scripts/CoA_finetune.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     collate_fn\u001b[39m=\u001b[39mcollator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bxuchang-lab0/mnt/sda/home/zijianwang/openvla/vla-scripts/CoA_finetune.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     num_workers\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,  \u001b[39m# Important =>> Set to 0 if using RLDS; TFDS rolls its own parallelism!\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxuchang-lab0/mnt/sda/home/zijianwang/openvla/vla-scripts/CoA_finetune.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxuchang-lab0/mnt/sda/home/zijianwang/openvla/vla-scripts/CoA_finetune.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLength of dataloader: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(dataloader)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataCollatorForCoASupervisedDataset' is not defined"
     ]
    }
   ],
   "source": [
    "collator = DataCollatorForCoASupervisedDataset(\n",
    "    processor.tokenizer.model_max_length, processor.tokenizer.pad_token_id, padding_side=\"right\"\n",
    ")\n",
    "dataloader = DataLoader(\n",
    "    episodic_vla_dataset,\n",
    "    collate_fn=collator,\n",
    "    sampler=None,\n",
    "    batch_size= 1,\n",
    "    num_workers=0,  # Important =>> Set to 0 if using RLDS; TFDS rolls its own parallelism!\n",
    ")\n",
    "print(f\"Length of dataloader: {len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm.tqdm(total=20000, leave=False) as progress:\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        progress.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:  # Infinite loop to keep reading data\n",
    "    for index, batch in tqdm.tqdm(enumerate(dataloader)):\n",
    "        lengths = batch['lengths']\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch['pixel_values'].shape)\n",
    "print(batch['input_ids'].shape)\n",
    "print(batch['labels'].shape)\n",
    "print(batch['dataset_names'])\n",
    "print(batch['lengths'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch['input_ids'][0])\n",
    "\n",
    "print(processor.tokenizer.batch_decode(batch['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# random.seed(42)  # Fix random seed\n",
    "start_index = random.randint(0, 110)\n",
    "print(start_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
